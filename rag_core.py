# rag_core.py
import os
import pickle
# import json
from dotenv import load_dotenv
import base64
# import fsspec
# --- LlamaIndex (ë©€í‹°ëª¨ë‹¬ RAGìš©) ---
# from llama_index.core import (
#     SimpleDirectoryReader,
#     StorageContext,
#     VectorStoreIndex,
#     Settings,
#     load_index_from_storage
# )
# from llama_index.readers.file import ImageReader, PDFReader # PDFì™€ ì´ë¯¸ì§€ ë¦¬ë”ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„í¬íŠ¸
# from llama_index.vector_stores.faiss import FaissVectorStore # FAISS ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©
# from llama_index.llms.google_genai import GoogleGenAI
# from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
# import faiss # FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ì„í¬íŠ¸


from langchain_core.messages import HumanMessage,AIMessageChunk
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader)
from langchain_community.vectorstores.faiss import FAISS
from langchain.storage import InMemoryStore
from langchain.retrievers import ParentDocumentRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from config import DOCS_DIR, KNOWLEDGE_BASE_DIR, SYSTEM_PROMPTS,LANG_TEXT,CONTEXTUALIZE_Q_PROMPTS
from langchain_core.prompts import MessagesPlaceholder
from config import (
    DOCS_DIR, KNOWLEDGE_BASE_DIR,
    PARENT_CHUNK_SIZE, PARENT_CHUNK_OVERLAP,
    CHILD_CHUNK_SIZE, CHILD_CHUNK_OVERLAP
)

load_dotenv()

def load_and_process_documents(llm: ChatGoogleGenerativeAI, directory: str):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë¬¸ì„œë¥¼ ëª¨ë‘ ë¡œë“œí•©ë‹ˆë‹¤.
    ì´ë¯¸ì§€ íŒŒì¼ì€ Vision LLMì„ í†µí•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    """
    all_documents = []
    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp']
    
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        file_ext = os.path.splitext(filename)[1].lower()

        # --- ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ ---
        if file_ext in image_extensions:
            if isinstance(llm, ChatGoogleGenerativeAI):
                print(f"ğŸ–¼ï¸  '{filename}' ì´ë¯¸ì§€ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
                description = describe_image_with_vision(llm, file_path)
                if description:
                    # ì´ë¯¸ì§€ ì„¤ëª…ì„ page_contentë¡œ, íŒŒì¼ëª…ì„ sourceë¡œ í•˜ëŠ” Document ê°ì²´ ìƒì„±
                    doc = Document(page_content=description, metadata={"source": filename})
                    all_documents.append(doc)
            else:
                 print(f"âš ï¸ '{filename}'ì€ ì´ë¯¸ì§€ íŒŒì¼ì´ì§€ë§Œ, í˜„ì¬ LLMì€ Visionì„ ì§€ì›í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # --- ê¸°ì¡´ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ---
        loader = None
        if file_ext == '.pdf':
            loader = PyPDFLoader(file_path)
        elif file_ext == '.docx':
            loader = UnstructuredWordDocumentLoader(file_path)
        elif file_ext == '.pptx':
            loader = UnstructuredPowerPointLoader(file_path)
        elif file_ext == '.txt':
            loader = TextLoader(file_path, encoding='utf-8')
        
        if loader:
            try:
                print(f"ğŸ“„ '{filename}' í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
                all_documents.extend(loader.load())
            except Exception as e:
                print(f"'{filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return all_documents
# def load_documents_from_directory(directory):
#     all_documents = []
#     for filename in os.listdir(directory):
#         file_path = os.path.join(directory, filename)
#         loader = None
#         if filename.lower().endswith('.pdf'):
#             loader = PyPDFLoader(file_path)
#         elif filename.lower().endswith('.docx'):
#             loader = UnstructuredWordDocumentLoader(file_path)
#         elif filename.lower().endswith('.pptx'):
#             loader = UnstructuredPowerPointLoader(file_path)
#         elif filename.lower().endswith('.txt'):
#             loader = TextLoader(file_path, encoding='utf-8')
#         if loader:
#             try:
#                 print(f"{filename} íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
#                 all_documents.extend(loader.load())
#             except Exception as e:
#                 print(f"'{filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#     return all_documents


def load_models(api_provider, api_key):
    if not api_key:
        return None, None
    try:
        if api_provider == 'NVIDIA':
            llm = ChatNVIDIA(
                model="mistralai/mixtral-8x7b-instruct-v0.1",
                nvidia_api_key=api_key
            )
            embedder = NVIDIAEmbeddings(
                model="nvidia/nv-embed-v1",
                nvidia_api_key=api_key
            )

        elif api_provider == 'Google':
            # LLMì€ Gemini ê·¸ëŒ€ë¡œ ì‚¬ìš©
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                google_api_key=api_key,
                timeout=120.0
            )
            
            embedder = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001", google_api_key=api_key)
            # í•„ìš”ì‹œ ë” ì •ë°€í•œ ëª¨ë¸:
            # embedder = OpenAIEmbeddings(model="text-embedding-3-large")

        else:
            return None, None

        return llm, embedder

    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None


def get_splitters():
    parent_splitter = RecursiveCharacterTextSplitter(
        chunk_size=PARENT_CHUNK_SIZE,
        chunk_overlap=PARENT_CHUNK_OVERLAP
    )
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHILD_CHUNK_SIZE,
        chunk_overlap=CHILD_CHUNK_OVERLAP
    )
    return parent_splitter, child_splitter


def create_and_save_retriever(llm,embedder, kb_name):
    if not os.path.exists(DOCS_DIR) or not os.listdir(DOCS_DIR):
        return None

    raw_documents = load_and_process_documents(llm,DOCS_DIR)
    if not raw_documents:
        return None

    parent_splitter, child_splitter = get_splitters()
    vectorstore = FAISS.from_documents(raw_documents, embedder)
    store = InMemoryStore()

    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
    )

    retriever.add_documents(raw_documents, ids=None)

    kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
    os.makedirs(kb_path, exist_ok=True)

    retriever.vectorstore.save_local(os.path.join(kb_path, "faiss_index"))
    with open(os.path.join(kb_path, "docstore.pkl"), "wb") as f:
        pickle.dump(retriever.docstore, f)

    return retriever


def load_retriever(embedder, kb_name):
    try:
        kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
        vectorstore = FAISS.load_local(
            os.path.join(kb_path, "faiss_index"),
            embedder,
            allow_dangerous_deserialization=True
        )
        with open(os.path.join(kb_path, "docstore.pkl"), "rb") as f:
            store = pickle.load(f)

        parent_splitter, child_splitter = get_splitters()
        return ParentDocumentRetriever(
            vectorstore=vectorstore,
            docstore=store,
            child_splitter=child_splitter,
            parent_splitter=parent_splitter,
        )
    except Exception as e:
        print(f"ë¦¬íŠ¸ë¦¬ë²„ '{kb_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
        return None


def update_and_save_retriever(llm,embedder, kb_name):
    # 1. ê¸°ì¡´ ë¦¬íŠ¸ë¦¬ë²„ì™€ ì»´í¬ë„ŒíŠ¸(vectorstore, docstore) ë¡œë“œ
    kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
    try:
        retriever = load_retriever(embedder, kb_name)
        if retriever is None: # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
            return create_and_save_retriever(llm,embedder, kb_name)
    except Exception:
        return create_and_save_retriever(llm,embedder, kb_name)

    # 2. ìƒˆë¡œ ì¶”ê°€í•  ë¬¸ì„œë§Œ ë¡œë“œ
    new_documents = load_and_process_documents(llm,DOCS_DIR)
    if not new_documents:
        print("ì¶”ê°€í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return retriever

    print(f"'{kb_name}'ì— {len(new_documents)}ê°œì˜ ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")

    # 3. ParentDocumentRetrieverì— ìƒˆ ë¬¸ì„œ ì¶”ê°€ (ì¤‘ìš”!)
    # add_documentsê°€ ë‚´ë¶€ì ìœ¼ë¡œ ìì‹ ì²­í¬ë¥¼ ë§Œë“¤ê³  ì„ë² ë”©í•˜ì—¬ vectorstoreì— ì¶”ê°€í•˜ê³ ,
    # ë¶€ëª¨ ë¬¸ì„œëŠ” docstoreì— ì €ì¥í•´ì¤ë‹ˆë‹¤.
    retriever.add_documents(new_documents, ids=None)

    # 4. ë³€ê²½ëœ vectorstoreì™€ docstoreë¥¼ ë‹¤ì‹œ ì €ì¥
    retriever.vectorstore.save_local(os.path.join(kb_path, "faiss_index"))
    with open(os.path.join(kb_path, "docstore.pkl"), "wb") as f:
        pickle.dump(retriever.docstore, f)

    return retriever


def create_rag_chain(llm, retriever, system_prompt):
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    chain = prompt_template | llm | StrOutputParser()
    return chain
# --- ğŸš¨ ìƒˆë¡œìš´ ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ ì¶”ê°€ ---

def create_conversational_rag_chain(llm, retriever, system_prompt, contextualize_q_system_prompt):
    """
    ëŒ€í™” ê¸°ë¡ì„ ì¸ì§€í•˜ëŠ” RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 1. ì»¨í…ìŠ¤íŠ¸í™” í”„ë¡¬í”„íŠ¸ (Contextualize Prompt)
    #    ğŸš¨ í•˜ë“œì½”ë”©ëœ ë¬¸ìì—´ ëŒ€ì‹ , ì¸ìë¡œ ë°›ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    # 2. íˆìŠ¤í† ë¦¬ ì¸ì§€ ë¦¬íŠ¸ë¦¬ë²„ (History-Aware Retriever) ìƒì„±
    #    ì´ ë¦¬íŠ¸ë¦¬ë²„ëŠ” ìœ„ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ê³ , ê·¸ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 3. ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (Answer Generation Prompt)
    #    ì´ í”„ë¡¬í”„íŠ¸ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
    qa_prompt = ChatPromptTemplate.from_messages([
    ("system", """Answer the user's question based on the following context and the chat history.

Context:
{context}"""), # <--- ë°”ë¡œ ì´ ë¶€ë¶„ì…ë‹ˆë‹¤! {context}ë¥¼ ìœ„í•œ ìë¦¬ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])
    # create_stuff_documents_chainì€ ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— 'ì±„ì›Œë„£ëŠ”(stuff)' ì²´ì¸ì…ë‹ˆë‹¤.
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # 4. ê²€ìƒ‰ ì²´ì¸ê³¼ ë‹µë³€ ìƒì„± ì²´ì¸ ê²°í•©
    #    ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ë  ëŒ€í™”í˜• RAG ì²´ì¸ì…ë‹ˆë‹¤.
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain


# --- ğŸš¨ ìƒˆë¡œìš´ ëŒ€í™”í˜• RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì¶”ê°€ ---

def get_response(user_input, chat_history, rag_chain):
    """
    ëŒ€í™”í˜• RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ê³¼ ì†ŒìŠ¤ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì²´ì¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤. chat_historyë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.
    response_stream = rag_chain.stream(
        {"input": user_input, "chat_history": chat_history}
    )
    
    # ğŸš¨ ìŠ¤íŠ¸ë¦¼ì—ì„œ ë„˜ì–´ì˜¤ëŠ” ë°ì´í„°ì˜ í˜•ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤. 'answer'ì™€ 'context' í‚¤ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.
    full_response = ""
    sources = []
    
    for response in response_stream:
        if "answer" in response:
            full_response += response["answer"]
            yield {"chunk": response["answer"]}
        if "context" in response and response["context"]:
            print("--- [ì°¸ê³ ëœ í…ìŠ¤íŠ¸] LANGCHAIN RAG CONTEXT ---")
            context_text = "\n\n---\n\n".join([doc.page_content for doc in response["context"]])
            print(context_text)
            print("--- [ì°¸ê³ ëœ í…ìŠ¤íŠ¸] ---")
            # ì†ŒìŠ¤ ì •ë³´ëŠ” ë§ˆì§€ë§‰ì— í•œ ë²ˆì— ë„˜ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
            sources = list(set([doc.metadata.get('source', 'Unknown') for doc in response["context"]]))
    
    # ìŠ¤íŠ¸ë¦¼ì´ ëë‚œ í›„, ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì†ŒìŠ¤ ì •ë³´ë¥¼ í•œ ë²ˆì— ì „ë‹¬í•©ë‹ˆë‹¤.
    yield {"sources": sources}

def get_contextual_response(user_input, retriever, chain):
    # --- ğŸš¨ ìˆ˜ì •: ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  í•¨ê»˜ yield í•˜ë„ë¡ ë³€ê²½ ---
    docs = retriever.invoke(user_input)
    
    # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ (ì¤‘ë³µ ì œê±° í¬í•¨)
    sources = list(set([doc.metadata.get('source', 'Unknown') for doc in docs]))
    sources.sort()

    # ì²« ë²ˆì§¸ yield: ê²€ìƒ‰ëœ ì†ŒìŠ¤ ì •ë³´ë¥¼ ë¨¼ì € ì „ë‹¬
    yield {"sources": sources}

    context = "\n\n".join([doc.page_content for doc in docs])
    augmented_user_input = f"Context: {context}\n\nQuestion: {user_input}\n"
    
    # ë‘ ë²ˆì§¸ yield: ê¸°ì¡´ì²˜ëŸ¼ ë‹µë³€ ìŠ¤íŠ¸ë¦¼ì„ ì „ë‹¬ (ì´ì œëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ì„œ)
    stream_iterator = chain.stream({"input": augmented_user_input})
    for chunk in stream_iterator:
        yield {"chunk": chunk}

# rag_core.py íŒŒì¼ ë§¨ ì•„ë˜ì— ì¶”ê°€

def image_to_base64(image_file):
    """Streamlitì˜ UploadedFile ê°ì²´(ì´ë¯¸ì§€)ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    image_file.seek(0)
    image_bytes = image_file.read()
    image_b64 = base64.b64encode(image_bytes).decode('utf-8')
    return image_b64

# --- Vision í•¨ìˆ˜ 1: íŒŒì¼ ì—…ë¡œë“œ(Base64) ë°©ì‹ ---
def get_response_with_vision_from_file(llm: ChatGoogleGenerativeAI, image_file, question: str, system_prompt: str):
    """RAG ê²€ìƒ‰ ì—†ì´, ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì§ì ‘ ë³´ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if not isinstance(llm, ChatGoogleGenerativeAI):
        warning_text = "Warning: Image analysis is only supported by Google (Gemini) models."
        yield AIMessageChunk(content=warning_text)
        return
    try:
        image_b64 = image_to_base64(image_file)
        image_b64 = image_b64.strip().replace('\n', '').replace('\r', '') #Rectification
        image_mime_type = image_file.type
        image_data_url = f"data:{image_mime_type};base64,{image_b64}"

        message = HumanMessage(
            content=[
                {"type": "text", "text": system_prompt},
                {"type": "text", "text": f"Question: {question}"},
                {"type": "image_url", "image_url": {"url": image_data_url}}
            ]
        )

        # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ì ìš© (ì—¬ê¸°ê°€ return llm.stream(...)ì„ ëŒ€ì²´í•©ë‹ˆë‹¤) ---
        print(f"--- [DEBUG_LLM_STREAM] Requesting LLM stream for model: {llm.model} (File Upload) ---")
        stream_iterator = llm.stream([message]) # LLM ìŠ¤íŠ¸ë¦¼ ê°ì²´ë¥¼ ë°›ìŒ
        
        first_chunk_received = False
        full_llm_debug_response = ""

        for i, chunk in enumerate(stream_iterator): 
            if not first_chunk_received:
                print(f"--- [DEBUG_LLM_STREAM] First chunk received! (Index: {i}) ---")
                first_chunk_received = True
            
            # chunk ê°ì²´ì˜ ì‹¤ì œ íƒ€ì…ê³¼ ë‚´ìš© í™•ì¸
            print(f"--- [DEBUG_LLM_STREAM] Chunk {i} Type: {type(chunk)}, Content (first 50 chars): {chunk.content[:50] if hasattr(chunk, 'content') else 'N/A'} ---")
            
            full_llm_debug_response += chunk.content if hasattr(chunk, 'content') else ""
            yield chunk # ì›ë˜ í•˜ë˜ ëŒ€ë¡œ Streamlitìœ¼ë¡œ ì²­í¬ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.

        if not first_chunk_received:
            print("--- [DEBUG_LLM_STREAM] No chunks received from LLM stream. ---")
        else:
            print(f"--- [DEBUG_LLM_STREAM] Full LLM response: \n{full_llm_debug_response} ---")
        # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ë ---

    except Exception as e:
        error_text = f"Error processing uploaded image: {e}"
        yield AIMessageChunk(content=error_text)
        return


# --- Vision í•¨ìˆ˜ 2: ê³µê°œ URL ë°©ì‹ ---
def get_response_with_vision_from_url(llm: ChatGoogleGenerativeAI, image_url: str, question: str, system_prompt: str):
    """RAG ê²€ìƒ‰ ì—†ì´, ê³µê°œ URLì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë¶„ì„í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if not isinstance(llm, ChatGoogleGenerativeAI):
        warning_text = "Warning: Image analysis is only supported by Google (Gemini) models. Please select Google as the AI provider in the sidebar."
        yield AIMessageChunk(content=warning_text)
        return
    
    if not image_url or not image_url.strip().startswith(("http://", "https://")):
        error_text = "Error: Please provide a valid URL starting with http:// or https://."
        yield AIMessageChunk(content=error_text)
        return

    message = HumanMessage(
        content=[
            {"type": "text", "text": system_prompt},
            {"type": "text", "text": f"Question: {question}"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ]
    )
    
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ì ìš© ---
    print(f"--- [DEBUG_LLM_STREAM] Requesting LLM stream for model: {llm.model} (URL) ---")
    stream_iterator = llm.stream([message])
    
    first_chunk_received = False
    full_llm_debug_response = ""

    for i, chunk in enumerate(stream_iterator): 
        if not first_chunk_received:
            print(f"--- [DEBUG_LLM_STREAM] First chunk received! (Index: {i}) ---")
            first_chunk_received = True
        
        print(f"--- [DEBUG_LLM_STREAM] Chunk {i} Type: {type(chunk)}, Content (first 50 chars): {chunk.content[:50] if hasattr(chunk, 'content') else 'N/A'} ---")
        
        full_llm_debug_response += chunk.content if hasattr(chunk, 'content') else ""
        yield chunk

    if not first_chunk_received:
        print("--- [DEBUG_LLM_STREAM] No chunks received from LLM stream. ---")
    else:
        print(f"--- [DEBUG_LLM_STREAM] Full LLM response: \n{full_llm_debug_response} ---")
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ë ---


# --- ìƒˆë¡œìš´ 'Vision + RAG' ìœµí•© í•¨ìˆ˜ ---
def get_fused_vision_rag_response(llm: ChatGoogleGenerativeAI, retriever, image_file, question: str, system_prompt: str):
    """
    1. Visionìœ¼ë¡œ ì´ë¯¸ì§€ì˜ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ê³ ,
    2. ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•œ ë’¤,
    3. ì´ë¯¸ì§€ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ìœµí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    """
    # --- 1ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ í•µì‹¬ ê°œë…(í‚¤ì›Œë“œ) ì¶”ì¶œ ---
    concept_extraction_prompt = """Analyze the provided image and identify the single most important technical concept or topic it represents. 
    Respond with ONLY that concept phrase, in 1 to 5 words. Do not add any explanation.
    Example responses: 'Ohm's Law', 'Kirchhoff's Current Law', 'Low Pass Filter', 'ThÃ©venin's theorem'.
    """
    
    # ê°œë… ì¶”ì¶œ ì „ìš©ìœ¼ë¡œ ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•¨).
    # ì£¼ì˜: llm.model_name ëŒ€ì‹  llm.modelì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ëª¨ë¸ IDë¥¼ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.
    concept_extractor_llm = ChatGoogleGenerativeAI(model=llm.model, google_api_key=llm.google_api_key)
    
    try:
        image_b64 = image_to_base64(image_file)
        image_mime_type = image_file.type
        image_data_url = f"data:{image_mime_type};base64,{image_b64}"

        concept_message = HumanMessage(
            content=[
                {"type": "text", "text": concept_extraction_prompt},
                {"type": "image_url", "image_url": {"url": image_data_url}}
            ]
        )
        extracted_concept = concept_extractor_llm.invoke([concept_message]).content.strip()
        print(f"--- [Vision->RAG] 1. Extracted Concept: '{extracted_concept}' ---")

    except Exception as e:
        yield AIMessageChunk(content=f"Error during concept extraction from image: {e}")
        return

    # --- 2ë‹¨ê³„: ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ë¬¸ì„œ ê²€ìƒ‰ ---
    try:
        # ğŸš¨ ê²€ìƒ‰ë˜ëŠ” ë¬¸ì„œ ìˆ˜ë¥¼ ì œí•œí•˜ì—¬ í† í° ì˜¤ë²„í”Œë¡œìš° ë°©ì§€ ë° ì‘ë‹µ ì†ë„ í–¥ìƒ
        retrieved_docs = retriever.invoke(extracted_concept, k=3) 
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
        sources = list(set([doc.metadata.get('source', 'Unknown') for doc in retrieved_docs]))
        sources.sort()
        # ì†ŒìŠ¤ ì •ë³´ë¥¼ ë¨¼ì € ì „ë‹¬
        yield {"sources": sources}
        retrieved_context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        print(f"--- [Vision->RAG] 2. Retrieved Context Length: {len(retrieved_context)} characters ---")
        
        if not retrieved_context:
            retrieved_context = "No relevant documents found in the knowledge base."
            
    except Exception as e:
        yield AIMessageChunk(content=f"Error during document retrieval with RAG: {e}")
        return

    # --- 3ë‹¨ê³„: ì´ë¯¸ì§€ + ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„± ---
    final_generation_prompt = system_prompt.format(context=retrieved_context)
    final_message = HumanMessage(
        content=[
            {"type": "text", "text": final_generation_prompt},
            {"type": "text", "text": f"User Question: {question}"},
            {"type": "image_url", "image_url": {"url": image_data_url}}
        ]
    )
    
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ì ìš© (ì—¬ê¸°ê°€ return llm.stream(...)ì„ ëŒ€ì²´í•©ë‹ˆë‹¤) ---
    print(f"--- [DEBUG_LLM_STREAM] Requesting LLM stream for model: {llm.model} (Fused Vision+RAG) ---")
    stream_iterator = llm.stream([final_message])
    
    first_chunk_received = False
    full_llm_debug_response = ""

    for i, chunk in enumerate(stream_iterator): 
        if not first_chunk_received:
            print(f"--- [DEBUG_LLM_STREAM] First chunk received! (Index: {i}) ---")
            first_chunk_received = True
        
        print(f"--- [DEBUG_LLM_STREAM] Chunk {i} Type: {type(chunk)}, Content (first 50 chars): {chunk.content[:50] if hasattr(chunk, 'content') else 'N/A'} ---")
        
        full_llm_debug_response += chunk.content if hasattr(chunk, 'content') else ""
        yield chunk

    if not first_chunk_received:
        print("--- [DEBUG_LLM_STREAM] No chunks received from LLM stream. ---")
    else:
        print(f"--- [DEBUG_LLM_STREAM] Full LLM response: \n{full_llm_debug_response} ---")
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ë ---
# rag_core.pyì˜ ë©€í‹°ëª¨ë‹¬ í•¨ìˆ˜ 3ê°œë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.


def describe_image_with_vision(llm: ChatGoogleGenerativeAI, image_path: str) -> str:
    """ì§€ì •ëœ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ Vision LLMì„ í†µí•´ ìƒì„¸í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # 1. MIME íƒ€ì… ì¶”ë¡  (íŒŒì¼ í™•ì¥ì ê¸°ë°˜)
        import mimetypes
        mime_type, _ = mimetypes.guess_type(image_path)
        if mime_type is None:
            mime_type = 'image/jpeg' # ê¸°ë³¸ê°’

        # 2. ì´ë¯¸ì§€ íŒŒì¼ì„ ì—´ê³  Base64ë¡œ ì¸ì½”ë”©
        with open(image_path, "rb") as image_file:
            image_b64 = base64.b64encode(image_file.read()).decode('utf-8')

        image_data_url = f"data:{mime_type};base64,{image_b64}"

        # 3. ì´ë¯¸ì§€ ì„¤ëª…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        prompt = """You are an expert in analyzing images. Describe the following image in detail, including all visible text, objects, scenes, and concepts. The description should be comprehensive and optimized for later text-based semantic search. Respond only with the description, without any introductory phrases."""

        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": image_data_url}}
            ]
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ í•œ ë²ˆì— ë‹µë³€ì„ ë°›ìŒ
        response = llm.invoke([message])
        print(response)
        return response.content
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({os.path.basename(image_path)}): {e}")
        return "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
# rag_core.py íŒŒì¼ ë§¨ ì•„ë˜ì— ì•„ë˜ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
import json

def stream_study_guide(llm, chat_history: list):
    """
    [ì´ë¯¸ì§€ ì œì™¸ ë²„ì „] í…ìŠ¤íŠ¸ ì±„íŒ… ê¸°ë¡ë§Œì„ ë¶„ì„í•˜ì—¬ Markdown í˜•ì‹ì˜ í•™ìŠµ ë…¸íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("ğŸ§  (í…ìŠ¤íŠ¸ ì „ìš©) í•™ìŠµ ë…¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1. [ìˆ˜ì •] ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì œì™¸í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ ëŒ€í™” ê¸°ë¡ë§Œìœ¼ë¡œ ì¬êµ¬ì„±
    history_for_prompt = []
    for msg in chat_history:
        role = "í•™ìƒ" if msg["role"] == "user" else "AI íŠœí„°"
        content = msg["content"]
        
        # ì´ë¯¸ì§€ì™€ ê´€ë ¨ëœ ë©”ì‹œì§€ëŠ” ê±´ë„ˆë›°ê±°ë‚˜ ë‚´ìš©ì—ì„œ ì´ë¯¸ì§€ ê´€ë ¨ ì–¸ê¸‰ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ í¬í•¨í•©ë‹ˆë‹¤.
        if content: # ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
            history_for_prompt.append(f'{role}: {content}')
    
    formatted_history = "\n".join(history_for_prompt)

    # 2. [ìˆ˜ì •] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ì„œ ì´ë¯¸ì§€ ê´€ë ¨ ëª¨ë“  ì§€ì‹œì‚¬í•­ ì œê±°
    system_prompt = f"""
You are an expert tutor creating a study guide. Analyze the provided conversation history between a student and an AI tutor. Your task is to generate a well-structured study guide in Markdown format based on the key topics discussed in the text.

The final output MUST strictly be in Markdown format and include the following sections:

1.  **# í•™ìŠµ ë…¸íŠ¸: [ëŒ€í™”ì˜ í•µì‹¬ ì£¼ì œ]**
    - ëŒ€í™”ì˜ ì „ì²´ ì£¼ì œë¥¼ ìš”ì•½í•˜ì—¬ ì œëª©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

2.  **## ğŸ“ í•µì‹¬ ê°œë… ìš”ì•½**
    - ëŒ€í™”ì—ì„œ ë‹¤ë£¨ì–´ì§„ ê°€ì¥ ì¤‘ìš”í•œ ê°œë…, ê³µì‹, ì›ë¦¬ ë“±ì„ 3~5ê°œì˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(bullet points)ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

3.  **## âœï¸ ë³µìŠµ í€´ì¦ˆ (3ë¬¸ì œ)**
    - ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, í•™ìƒì´ ìì‹ ì˜ ì´í•´ë„ë¥¼ ì ê²€í•  ìˆ˜ ìˆëŠ” ê°ê´€ì‹ ë˜ëŠ” ë‹¨ë‹µí˜• ë¬¸ì œ 3ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    í˜•ì‹:
        **[ë¬¸ì œ 1]** (ì§ˆë¬¸ ë‚´ìš©)
        **[ë¬¸ì œ 2]** (ì§ˆë¬¸ ë‚´ìš©)
        **[ë¬¸ì œ 3]** (ì§ˆë¬¸ ë‚´ìš©)...
        ...
        ----------------------
        **[ë¬¸ì œ 1]**
        <ì •ë‹µ ë° í•´ì„¤>
        (ë‹µë³€ ë‚´ìš©)
        **[ë¬¸ì œ 2]** 
        <ì •ë‹µ ë° í•´ì„¤>
        (ë‹µë³€ ë‚´ìš©)

Please generate the entire study guide based on the conversation below.

---
[ëŒ€í™” ê¸°ë¡]
{formatted_history}
---
"""
    
    try:
        # invoke ëŒ€ì‹  streamì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        stream = llm.stream(system_prompt)
        for chunk in stream:
            yield chunk.content # content ë¶€ë¶„ë§Œ yield
        print("âœ… í•™ìŠµ ë…¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ í•™ìŠµ ë…¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        yield f"í•™ìŠµ ë…¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

from fpdf import FPDF


def save_markdown_to_pdf(markdown_content: str) -> bytes:
    """
    [fpdf2 ìµœì¢… ë²„ì „ - ë ˆì´ì•„ì›ƒ ìˆ˜ì •] Markdown í…ìŠ¤íŠ¸ë¥¼ ì•ˆì •ì ì¸ PDFë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    í”„ë¡œì íŠ¸ ë‚´ë¶€ì— í¬í•¨ëœ í•œê¸€ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print("ğŸ“„ [fpdf2] Markdownì„ PDFë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")

    font_dir = "fonts"
    regular_font_path = os.path.join(font_dir, "NotoSansKR-Regular.ttf")
    bold_font_path = os.path.join(font_dir, "NotoSansKR-Bold.ttf")

    pdf = FPDF()
    pdf.add_page()
    font_family = "NotoSansKR"

    try:
        if not os.path.exists(regular_font_path) or not os.path.exists(bold_font_path):
             raise FileNotFoundError("í°íŠ¸ íŒŒì¼('NotoSansKR-Regular.ttf' ë˜ëŠ” 'NotoSansKR-Bold.ttf')ì„ 'fonts' í´ë”ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        pdf.add_font(font_family, "", regular_font_path, uni=True)
        pdf.add_font(font_family, "B", bold_font_path, uni=True)
        pdf.set_font(font_family, size=11)
        
    except Exception as e:
        print(f"âš ï¸ ê²½ê³ : í•œê¸€ í°íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        pdf.set_font("helvetica", size=11)
        font_family = "helvetica"

    # --- â–¼â–¼â–¼ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ (ëª¨ë“  multi_cellì— ln=1 ì¶”ê°€) â–¼â–¼â–¼ ---
    for line in markdown_content.split('\n'):
        line = line.strip()
        if line.startswith('# '):
            pdf.set_font(font_family, 'B', size=24)
            pdf.multi_cell(0, 12, line.replace('# ', '').strip(), ln=1)
            pdf.ln(5) # ì œëª© ì•„ë˜ì— ì¶”ê°€ ê°„ê²©
        elif line.startswith('## '):
            pdf.set_font(font_family, 'B', size=18)
            pdf.multi_cell(0, 10, line.replace('## ', '').strip(), ln=1)
            pdf.ln(3)
        elif line.startswith('### '):
            pdf.set_font(font_family, 'B', size=14)
            pdf.multi_cell(0, 10, line.replace('### ', '').strip(), ln=1)
            pdf.ln(1)
        elif line.startswith('* ') or line.startswith('- '):
            pdf.set_font(font_family, '', size=11)
            pdf.multi_cell(0, 7, f"  â€¢ {line[2:].strip()}", ln=1)
        elif line.startswith('<ì •ë‹µ ë° í•´ì„¤>'):
             pdf.set_font(font_family, 'B', size=11)
             pdf.multi_cell(0, 7, line, ln=1)
        elif line == "": # ë¹ˆ ì¤„ì´ë©´ ê°„ê²©ì„ ì¡°ê¸ˆ ë„ì›€
            pdf.ln(3)
        else:
            pdf.set_font(font_family, '', size=11)
            pdf.multi_cell(0, 7, line, ln=1)

    print("âœ… [fpdf2] PDF ë³€í™˜ ì™„ë£Œ.")
    return bytes(pdf.output(dest='S'))














# --- âœ¨ [ìˆ˜ì •] ë©€í‹°ëª¨ë‹¬ DB ìƒì„±, ë¡œë“œ, ì—…ë°ì´íŠ¸ ë¡œì§ ì™„ì „ êµì²´ ---

# --- âœ¨ [ìˆ˜ì •] ë©€í‹°ëª¨ë‹¬ DB ìƒì„±, ë¡œë“œ, ì—…ë°ì´íŠ¸ ë¡œì§ (FAISS í†µì¼ ìµœì¢…ë³¸) ---

# def is_valid_json(path):
#     try:
#         with open(path, "r", encoding="utf-8") as f:
#             json.load(f)
#         return True
#     except Exception:
#         return False


    
# def _get_metadata_path(kb_path):
#     return os.path.join(kb_path, "index_metadata.json")


# def _setup_settings(api_key: str):
#     """API í‚¤ë¥¼ ë°›ì•„ LlamaIndex ì „ì—­ ì„¤ì •ì„ êµ¬ì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
#     Settings.llm = GoogleGenAI(model_name="models/gemini-1.5-pro-latest", api_key=api_key)
#     Settings.embed_model = GoogleGenAIEmbedding(model_name="models/embedding-001", api_key=api_key)

# def _get_document_reader():
#     """PDFì™€ ì´ë¯¸ì§€ íŒŒì¼ íŒŒì„œë¥¼ í¬í•¨í•˜ëŠ” SimpleDirectoryReaderë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
#     return SimpleDirectoryReader(
#         DOCS_DIR,
#         file_extractor={
#             ".pdf": PDFReader(),
#             ".png": ImageReader(parse_text=True),
#             ".jpg": ImageReader(parse_text=True),
#             ".jpeg": ImageReader(parse_text=True),
#         }
#     )

# def create_multimodal_index(kb_name: str, api_key: str):
#     """
#     [ìˆ˜ì •ë¨] ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , FAISS ë²¡í„° ì €ì¥ì†Œì™€ í•¨ê»˜ VectorStoreIndexë¥¼ ìƒì„± í›„ í•œ ë²ˆì— ì €ì¥í•©ë‹ˆë‹¤.
#     """
#     kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
#     persist_path = os.path.join(kb_path, "multimodal_db")
#     os.makedirs(persist_path, exist_ok=True)

#     _setup_settings(api_key)

#     documents = _get_document_reader().load_data()
#     if not documents:
#         print("âš ï¸ ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
#         return

#     fs = fsspec.filesystem("file")

#     # 2. FAISS ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
#     d = 768
#     faiss_index = faiss.IndexFlatL2(d)
#     vector_store = FaissVectorStore(faiss_index=faiss_index)

#     # 3. StorageContextë¥¼ ìƒì„±í•  ë•Œ, ìœ„ì—ì„œ ë§Œë“  fs ê°ì²´ë¥¼ ì „ë‹¬
#     storage_context = StorageContext.from_defaults(
#         vector_store=vector_store, 
#         fs=fs
#     )
    
#     # 4. ë³€í™˜ëœ ë…¸ë“œë¡œë¶€í„° ì¸ë±ìŠ¤ ìƒì„±
#     index = VectorStoreIndex(storage_context=storage_context)

#     # 5. ì¸ë±ìŠ¤ ì €ì¥ (ì´ì œ fsì— ì„¤ì •ëœ UTF-8 ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ê²Œ ë¨)
#     index.storage_context.persist(persist_dir=persist_path)
#     print(f"âœ… '{kb_name}' ë©€í‹°ëª¨ë‹¬ ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ. ê²½ë¡œ: {persist_path}")


# def load_multimodal_query_engine(kb_name: str, api_key: str):
#     """
#     [ìˆ˜ì •ë¨] ì§€ì •ëœ ê²½ë¡œì—ì„œ ì „ì²´ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ì—¬ ì¿¼ë¦¬ ì—”ì§„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
#     persist_path = os.path.join(kb_path, "multimodal_db")

#     if not os.path.exists(persist_path):
#         print(f"âš ï¸ '{kb_name}'ì— ëŒ€í•œ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
#         # UIì—ì„œ ìƒˆë¡œ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ê¸° ìœ„í•´ None ë°˜í™˜ ë˜ëŠ” ì—ëŸ¬ ë°œìƒ
#         return None

#     _setup_settings(api_key)

#     try:
#         print(f"ğŸ“‚ '{kb_name}' ë©€í‹°ëª¨ë‹¬ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
#         # --- â–¼â–¼â–¼ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ â–¼â–¼â–¼ ---
#         # 1. UTF-8 ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ê°ì²´ ìƒì„±
#         fs = fsspec.filesystem("file")

#         # 2. StorageContextë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œë„ fs ê°ì²´ë¥¼ ì „ë‹¬
#         storage_context = StorageContext.from_defaults(persist_dir=persist_path, fs=fs)
        
#         # 3. ìˆ˜ì •ëœ storage_contextë¡œ ì¸ë±ìŠ¤ ë¡œë“œ
#         index = load_index_from_storage(storage_context)
#         # --- â–²â–²â–² í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ë â–²â–²â–² ---
#         print("âœ… ë©€í‹°ëª¨ë‹¬ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ.")
#         return index.as_query_engine(streaming=True) # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
#     except Exception as e:
#         print(f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         print("â„¹ï¸ ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ì†ìƒë˜ì—ˆê±°ë‚˜ í˜„ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ê³¼ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
#         print("â„¹ï¸ ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ìƒì„±í•´ ë³´ì„¸ìš”.")
#         return None

# # ì°¸ê³ : update_multimodal_index í•¨ìˆ˜ëŠ” ì œê³µëœ ì½”ë“œì˜ ë¡œì§ì´ ì´ë¯¸ íš¨ìœ¨ì ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.
# # ë‹¨, createì™€ load í•¨ìˆ˜ëŠ” ìœ„ì˜ ìˆ˜ì •ëœ ë²„ì „ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
# def _get_metadata_path(kb_path):
#     return os.path.join(kb_path, "index_metadata.json")


# def _load_metadata(kb_path):
#     meta_path = _get_metadata_path(kb_path)
#     if not os.path.exists(meta_path):
#         return {}
#     try:
#         with open(meta_path, "r", encoding="utf-8") as f:
#             return json.load(f)
#     except json.JSONDecodeError:
#         return {}


# def _save_metadata(kb_path, metadata):
#     with open(_get_metadata_path(kb_path), "w", encoding="utf-8") as f:
#         json.dump(metadata, f, indent=2, ensure_ascii=False)


# def update_multimodal_index(kb_name: str, api_key: str):
#     kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
#     persist_path = os.path.join(kb_path, "multimodal_db")
#     os.makedirs(persist_path, exist_ok=True)

#     _setup_settings(api_key)

#     prev_meta = _load_metadata(kb_path)
#     all_files = [
#         os.path.join(DOCS_DIR, f)
#         for f in os.listdir(DOCS_DIR)
#         if f.lower().endswith((".pdf", ".png", ".jpg", ".jpeg"))
#     ]

#     new_or_modified = []
#     for file_path in all_files:
#         mtime = os.path.getmtime(file_path)
#         if file_path not in prev_meta or prev_meta[file_path] < mtime:
#             new_or_modified.append(file_path)

#     if not new_or_modified:
#         print("âœ… ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
#         return

#     print(f"ğŸ§© {len(new_or_modified)}ê°œì˜ ìƒˆ(ë˜ëŠ” ìˆ˜ì •ëœ) ë¬¸ì„œë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤...")

#     try:
#         storage_context = StorageContext.from_defaults(persist_dir=persist_path)
#         index = load_index_from_storage(storage_context)
#         print("ğŸ“‚ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
#     except Exception as e:
#         print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {e}")
#         return create_multimodal_index(kb_name, api_key)

#     docs_to_add = SimpleDirectoryReader(
#         input_files=new_or_modified
#     ).load_data()

#     for doc in docs_to_add:
#         index.insert(doc)

#     index.storage_context.persist(persist_dir=persist_path)
#     for f in new_or_modified:
#         prev_meta[f] = os.path.getmtime(f)
#     _save_metadata(kb_path, prev_meta)
#     print(f"âœ… '{kb_name}' ë©€í‹°ëª¨ë‹¬ ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


#ê¸°ì¡´ stream ë°˜í™˜ ë¡œì§ ë¬¸ì œë¥¼ chuckë¡œ í•´ê²°í•¨ ë­ê°€ ë­”ì§€.... ì¶”ê°€ì ì¸ ì´í•´ ì„¤ëª… í•„ìˆ˜
# def image_to_base64(image_file):
#     """Streamlitì˜ UploadedFile ê°ì²´(ì´ë¯¸ì§€)ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
#     image_file.seek(0) # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤ (ì¬ì‚¬ìš© ëŒ€ë¹„)
#     image_bytes = image_file.read()
#     image_b64 = base64.b64encode(image_bytes).decode('utf-8')
#     return image_b64


# # --- Vision í•¨ìˆ˜ 1: íŒŒì¼ ì—…ë¡œë“œ(Base64) ë°©ì‹ (MIME íƒ€ì… ë™ì  ì²˜ë¦¬ë¡œ ìˆ˜ì •) ---
# def get_response_with_vision_from_file(llm: ChatGoogleGenerativeAI, image_file, question: str, system_prompt: str):
#     """RAG ê²€ìƒ‰ ì—†ì´, ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì§ì ‘ ë³´ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤."""
#     if not isinstance(llm, ChatGoogleGenerativeAI):
#         warning_text = "Warning: Image analysis is only supported by Google (Gemini) models."
#         yield AIMessageChunk(content=warning_text)
#         return
#     try:
#         # 1. ì´ë¯¸ì§€ë¥¼ Base64 ë¬¸ìì—´ë¡œ ì¸ì½”ë”©
#         image_b64 = image_to_base64(image_file)
#         image_b64 = image_b64.strip().replace('\n', '').replace('\r', '') #Rectification
        
#         # 2. ğŸŸ¢ Streamlitì˜ UploadedFile ê°ì²´ì—ì„œ ì‹¤ì œ MIME íƒ€ì… ê°€ì ¸ì˜¤ê¸°
#         image_mime_type = image_file.type # ì˜ˆ: 'image/png' ë˜ëŠ” 'image/jpeg'
        
#         # 3. ğŸŸ¢ Data URL í˜•ì‹ìœ¼ë¡œ ì¡°í•©
#         image_data_url = f"data:{image_mime_type};base64,{image_b64}"

#         # 4. ğŸŸ¢ ì¡°í•©ëœ Data URLì„ ë©”ì‹œì§€ì— í¬í•¨
#         message = HumanMessage(
#             content=[
#                 {"type": "text", "text": system_prompt},
#                 {"type": "text", "text": f"Question: {question}"},
#                 {"type": "image_url", "image_url": {"url": image_data_url}}
#             ]
#         )
#         return llm.stream([message])
#     except Exception as e:
#         error_text = f"Error processing uploaded image: {e}"
#         yield AIMessageChunk(content=error_text)
#         return


# def get_response_with_vision_from_url(llm: ChatGoogleGenerativeAI, image_url: str, question: str, system_prompt: str):
#     """
#     RAG ê²€ìƒ‰ ì—†ì´, ê³µê°œ URLì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë¶„ì„í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤.
#     """
#     if not isinstance(llm, ChatGoogleGenerativeAI):
#         warning_text = "Warning: Image analysis is only supported by Google (Gemini) models. Please select Google as the AI provider in the sidebar."
#         yield AIMessageChunk(content=warning_text)
#         return
    
#     if not image_url or not image_url.strip().startswith(("http://", "https://")):
#         error_text = "Error: Please provide a valid URL starting with http:// or https://."
#         yield AIMessageChunk(content=error_text)
#         return

#     message = HumanMessage(
#         content=[
#             {"type": "text", "text": system_prompt},
#             {"type": "text", "text": f"Question: {question}"},
#             {
#                 "type": "image_url",
#                 "image_url": {
#                     "url": image_url
#                 }
#             }
#         ]
#     )
    
#     return llm.stream([message])
# # --- ìƒˆë¡œìš´ 'Vision + RAG' ìœµí•© í•¨ìˆ˜ ---
# def get_fused_vision_rag_response(llm: ChatGoogleGenerativeAI, retriever, image_file, question: str, system_prompt: str):
#     """
#     1. Visionìœ¼ë¡œ ì´ë¯¸ì§€ì˜ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ê³ ,
#     2. ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•œ ë’¤,
#     3. ì´ë¯¸ì§€ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ìœµí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
#     """
#     # --- 1ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ í•µì‹¬ ê°œë…(í‚¤ì›Œë“œ) ì¶”ì¶œ ---
#     concept_extraction_prompt = """Analyze the provided image and identify the single most important technical concept or topic it represents. 
#     Respond with ONLY that concept phrase, in 1 to 5 words. Do not add any explanation.
#     Example responses: 'Ohm's Law', 'Kirchhoff's Current Law', 'Low Pass Filter', 'ThÃ©venin's theorem'.
#     """
    
#     # ê°œë… ì¶”ì¶œ ì „ìš©ìœ¼ë¡œ ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•¨).
#     concept_extractor_llm = ChatGoogleGenerativeAI(model=llm.model, google_api_key=llm.google_api_key)
    
#     try:
#         image_b64 = image_to_base64(image_file)
#         image_mime_type = image_file.type
#         image_data_url = f"data:{image_mime_type};base64,{image_b64}"

#         concept_message = HumanMessage(
#             content=[
#                 {"type": "text", "text": concept_extraction_prompt},
#                 {"type": "image_url", "image_url": {"url": image_data_url}}
#             ]
#         )
#         # .invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— ë°›ìŠµë‹ˆë‹¤.
#         extracted_concept = concept_extractor_llm.invoke([concept_message]).content.strip()
        
#         # í„°ë¯¸ë„ì— ì¶”ì¶œëœ ê°œë…ì„ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…
#         print(f"--- [Vision->RAG] 1. Extracted Concept: '{extracted_concept}' ---")

#     except Exception as e:
#         yield AIMessageChunk(content=f"Error during concept extraction from image: {e}")
#         return

#     # --- 2ë‹¨ê³„: ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ë¬¸ì„œ ê²€ìƒ‰ ---
#     try:
#         retrieved_docs = retriever.invoke(extracted_concept)
#         retrieved_context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
#         # í„°ë¯¸ë„ì— ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…
#         print(f"--- [Vision->RAG] 2. Retrieved Context Length: {len(retrieved_context)} characters ---")
        
#         if not retrieved_context:
#             retrieved_context = "No relevant documents found in the knowledge base."
            
#     except Exception as e:
#         yield AIMessageChunk(content=f"Error during document retrieval with RAG: {e}")
#         return

#     # --- 3ë‹¨ê³„: ì´ë¯¸ì§€ + ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„± ---
#     # system_prompt (ì˜ˆ: "You are a brilliant engineering problem solver...")ë¥¼ ì—¬ê¸°ì— í†µí•©í•©ë‹ˆë‹¤.
#     final_generation_prompt = f"""{system_prompt}

# You MUST use the following 'Retrieved Documents' as the primary source of truth to explain the concepts related to the image in your answer. Synthesize the information from the documents and the image to provide a comprehensive and accurate response.

# [Retrieved Documents]
# {retrieved_context}
# """

#     final_message = HumanMessage(
#         content=[
#             {"type": "text", "text": final_generation_prompt},
#             {"type": "text", "text": f"User Question: {question}"},
#             {"type": "image_url", "image_url": {"url": image_data_url}}
#         ]
#     )
    
#     # ìµœì¢… ë‹µë³€ì€ ì‚¬ìš©ìì—ê²Œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬
#     print("--- [Vision->RAG] 3. Generating final fused response... ---")
#     return llm.stream([final_message])
    

