# rag_core.py
import os
import pickle
from dotenv import load_dotenv
import base64
from langchain_core.messages import HumanMessage,AIMessageChunk
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader)
from langchain_community.vectorstores.faiss import FAISS
from langchain.storage import InMemoryStore
from langchain.retrievers import ParentDocumentRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from config import DOCS_DIR, KNOWLEDGE_BASE_DIR, SYSTEM_PROMPTS,LANG_TEXT,CONTEXTUALIZE_Q_PROMPTS
from langchain_core.prompts import MessagesPlaceholder
from config import (
    DOCS_DIR, KNOWLEDGE_BASE_DIR,
    PARENT_CHUNK_SIZE, PARENT_CHUNK_OVERLAP,
    CHILD_CHUNK_SIZE, CHILD_CHUNK_OVERLAP
)

load_dotenv()


def load_documents_from_directory(directory):
    all_documents = []
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        loader = None
        if filename.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif filename.lower().endswith('.docx'):
            loader = UnstructuredWordDocumentLoader(file_path)
        elif filename.lower().endswith('.pptx'):
            loader = UnstructuredPowerPointLoader(file_path)
        elif filename.lower().endswith('.txt'):
            loader = TextLoader(file_path, encoding='utf-8')
        if loader:
            try:
                print(f"{filename} íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
                all_documents.extend(loader.load())
            except Exception as e:
                print(f"'{filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return all_documents


def load_models(api_provider, api_key):
    if not api_key:
        return None, None
    try:
        if api_provider == 'NVIDIA':
            llm = ChatNVIDIA(
                model="mistralai/mixtral-8x7b-instruct-v0.1",
                nvidia_api_key=api_key
            )
            embedder = NVIDIAEmbeddings(
                model="nvidia/nv-embed-v1",
                nvidia_api_key=api_key
            )

        elif api_provider == 'Google':
            # LLMì€ Gemini ê·¸ëŒ€ë¡œ ì‚¬ìš©
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                google_api_key=api_key,
                timeout=120.0
            )
            
            embedder = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001", google_api_key=api_key)
            # í•„ìš”ì‹œ ë” ì •ë°€í•œ ëª¨ë¸:
            # embedder = OpenAIEmbeddings(model="text-embedding-3-large")

        else:
            return None, None

        return llm, embedder

    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None


def get_splitters():
    parent_splitter = RecursiveCharacterTextSplitter(
        chunk_size=PARENT_CHUNK_SIZE,
        chunk_overlap=PARENT_CHUNK_OVERLAP
    )
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHILD_CHUNK_SIZE,
        chunk_overlap=CHILD_CHUNK_OVERLAP
    )
    return parent_splitter, child_splitter


def create_and_save_retriever(embedder, kb_name):
    if not os.path.exists(DOCS_DIR) or not os.listdir(DOCS_DIR):
        return None

    raw_documents = load_documents_from_directory(DOCS_DIR)
    if not raw_documents:
        return None

    parent_splitter, child_splitter = get_splitters()
    vectorstore = FAISS.from_documents(raw_documents, embedder)
    store = InMemoryStore()

    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
    )

    retriever.add_documents(raw_documents, ids=None)

    kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
    os.makedirs(kb_path, exist_ok=True)

    retriever.vectorstore.save_local(os.path.join(kb_path, "faiss_index"))
    with open(os.path.join(kb_path, "docstore.pkl"), "wb") as f:
        pickle.dump(retriever.docstore, f)

    return retriever


def load_retriever(embedder, kb_name):
    try:
        kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
        vectorstore = FAISS.load_local(
            os.path.join(kb_path, "faiss_index"),
            embedder,
            allow_dangerous_deserialization=True
        )
        with open(os.path.join(kb_path, "docstore.pkl"), "rb") as f:
            store = pickle.load(f)

        parent_splitter, child_splitter = get_splitters()
        return ParentDocumentRetriever(
            vectorstore=vectorstore,
            docstore=store,
            child_splitter=child_splitter,
            parent_splitter=parent_splitter,
        )
    except Exception as e:
        print(f"ë¦¬íŠ¸ë¦¬ë²„ '{kb_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
        return None


def update_and_save_retriever(embedder, kb_name):
    # 1. ê¸°ì¡´ ë¦¬íŠ¸ë¦¬ë²„ì™€ ì»´í¬ë„ŒíŠ¸(vectorstore, docstore) ë¡œë“œ
    kb_path = os.path.join(KNOWLEDGE_BASE_DIR, kb_name)
    try:
        retriever = load_retriever(embedder, kb_name)
        if retriever is None: # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
            return create_and_save_retriever(embedder, kb_name)
    except Exception:
        return create_and_save_retriever(embedder, kb_name)

    # 2. ìƒˆë¡œ ì¶”ê°€í•  ë¬¸ì„œë§Œ ë¡œë“œ
    new_documents = load_documents_from_directory(DOCS_DIR)
    if not new_documents:
        print("ì¶”ê°€í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return retriever

    print(f"'{kb_name}'ì— {len(new_documents)}ê°œì˜ ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")

    # 3. ParentDocumentRetrieverì— ìƒˆ ë¬¸ì„œ ì¶”ê°€ (ì¤‘ìš”!)
    # add_documentsê°€ ë‚´ë¶€ì ìœ¼ë¡œ ìì‹ ì²­í¬ë¥¼ ë§Œë“¤ê³  ì„ë² ë”©í•˜ì—¬ vectorstoreì— ì¶”ê°€í•˜ê³ ,
    # ë¶€ëª¨ ë¬¸ì„œëŠ” docstoreì— ì €ì¥í•´ì¤ë‹ˆë‹¤.
    retriever.add_documents(new_documents, ids=None)

    # 4. ë³€ê²½ëœ vectorstoreì™€ docstoreë¥¼ ë‹¤ì‹œ ì €ì¥
    retriever.vectorstore.save_local(os.path.join(kb_path, "faiss_index"))
    with open(os.path.join(kb_path, "docstore.pkl"), "wb") as f:
        pickle.dump(retriever.docstore, f)

    return retriever


def create_rag_chain(llm, retriever, system_prompt):
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    chain = prompt_template | llm | StrOutputParser()
    return chain
# --- ğŸš¨ ìƒˆë¡œìš´ ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ ì¶”ê°€ ---

def create_conversational_rag_chain(llm, retriever, system_prompt, contextualize_q_system_prompt):
    """
    ëŒ€í™” ê¸°ë¡ì„ ì¸ì§€í•˜ëŠ” RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 1. ì»¨í…ìŠ¤íŠ¸í™” í”„ë¡¬í”„íŠ¸ (Contextualize Prompt)
    #    ğŸš¨ í•˜ë“œì½”ë”©ëœ ë¬¸ìì—´ ëŒ€ì‹ , ì¸ìë¡œ ë°›ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    # 2. íˆìŠ¤í† ë¦¬ ì¸ì§€ ë¦¬íŠ¸ë¦¬ë²„ (History-Aware Retriever) ìƒì„±
    #    ì´ ë¦¬íŠ¸ë¦¬ë²„ëŠ” ìœ„ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ê³ , ê·¸ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 3. ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (Answer Generation Prompt)
    #    ì´ í”„ë¡¬í”„íŠ¸ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
    qa_prompt = ChatPromptTemplate.from_messages([
    ("system", """Answer the user's question based on the following context and the chat history.

Context:
{context}"""), # <--- ë°”ë¡œ ì´ ë¶€ë¶„ì…ë‹ˆë‹¤! {context}ë¥¼ ìœ„í•œ ìë¦¬ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])
    # create_stuff_documents_chainì€ ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— 'ì±„ì›Œë„£ëŠ”(stuff)' ì²´ì¸ì…ë‹ˆë‹¤.
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # 4. ê²€ìƒ‰ ì²´ì¸ê³¼ ë‹µë³€ ìƒì„± ì²´ì¸ ê²°í•©
    #    ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ë  ëŒ€í™”í˜• RAG ì²´ì¸ì…ë‹ˆë‹¤.
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain


# --- ğŸš¨ ìƒˆë¡œìš´ ëŒ€í™”í˜• RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì¶”ê°€ ---

def get_response(user_input, chat_history, rag_chain):
    """
    ëŒ€í™”í˜• RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ê³¼ ì†ŒìŠ¤ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì²´ì¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤. chat_historyë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.
    response_stream = rag_chain.stream(
        {"input": user_input, "chat_history": chat_history}
    )
    
    # ğŸš¨ ìŠ¤íŠ¸ë¦¼ì—ì„œ ë„˜ì–´ì˜¤ëŠ” ë°ì´í„°ì˜ í˜•ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤. 'answer'ì™€ 'context' í‚¤ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.
    full_response = ""
    sources = []
    
    for response in response_stream:
        if "answer" in response:
            full_response += response["answer"]
            yield {"chunk": response["answer"]}
        if "context" in response and response["context"]:
            # ì†ŒìŠ¤ ì •ë³´ëŠ” ë§ˆì§€ë§‰ì— í•œ ë²ˆì— ë„˜ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
            sources = list(set([doc.metadata.get('source', 'Unknown') for doc in response["context"]]))
    
    # ìŠ¤íŠ¸ë¦¼ì´ ëë‚œ í›„, ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì†ŒìŠ¤ ì •ë³´ë¥¼ í•œ ë²ˆì— ì „ë‹¬í•©ë‹ˆë‹¤.
    yield {"sources": sources}

def get_contextual_response(user_input, retriever, chain):
    # --- ğŸš¨ ìˆ˜ì •: ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  í•¨ê»˜ yield í•˜ë„ë¡ ë³€ê²½ ---
    docs = retriever.invoke(user_input)
    
    # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ (ì¤‘ë³µ ì œê±° í¬í•¨)
    sources = list(set([doc.metadata.get('source', 'Unknown') for doc in docs]))
    sources.sort()

    # ì²« ë²ˆì§¸ yield: ê²€ìƒ‰ëœ ì†ŒìŠ¤ ì •ë³´ë¥¼ ë¨¼ì € ì „ë‹¬
    yield {"sources": sources}

    context = "\n\n".join([doc.page_content for doc in docs])
    augmented_user_input = f"Context: {context}\n\nQuestion: {user_input}\n"
    
    # ë‘ ë²ˆì§¸ yield: ê¸°ì¡´ì²˜ëŸ¼ ë‹µë³€ ìŠ¤íŠ¸ë¦¼ì„ ì „ë‹¬ (ì´ì œëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ì„œ)
    stream_iterator = chain.stream({"input": augmented_user_input})
    for chunk in stream_iterator:
        yield {"chunk": chunk}

# rag_core.py íŒŒì¼ ë§¨ ì•„ë˜ì— ì¶”ê°€

def image_to_base64(image_file):
    """Streamlitì˜ UploadedFile ê°ì²´(ì´ë¯¸ì§€)ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    image_file.seek(0)
    image_bytes = image_file.read()
    image_b64 = base64.b64encode(image_bytes).decode('utf-8')
    return image_b64

# --- Vision í•¨ìˆ˜ 1: íŒŒì¼ ì—…ë¡œë“œ(Base64) ë°©ì‹ ---
def get_response_with_vision_from_file(llm: ChatGoogleGenerativeAI, image_file, question: str, system_prompt: str):
    """RAG ê²€ìƒ‰ ì—†ì´, ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì§ì ‘ ë³´ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if not isinstance(llm, ChatGoogleGenerativeAI):
        warning_text = "Warning: Image analysis is only supported by Google (Gemini) models."
        yield AIMessageChunk(content=warning_text)
        return
    try:
        image_b64 = image_to_base64(image_file)
        image_b64 = image_b64.strip().replace('\n', '').replace('\r', '') #Rectification
        image_mime_type = image_file.type
        image_data_url = f"data:{image_mime_type};base64,{image_b64}"

        message = HumanMessage(
            content=[
                {"type": "text", "text": system_prompt},
                {"type": "text", "text": f"Question: {question}"},
                {"type": "image_url", "image_url": {"url": image_data_url}}
            ]
        )

        # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ì ìš© (ì—¬ê¸°ê°€ return llm.stream(...)ì„ ëŒ€ì²´í•©ë‹ˆë‹¤) ---
        print(f"--- [DEBUG_LLM_STREAM] Requesting LLM stream for model: {llm.model} (File Upload) ---")
        stream_iterator = llm.stream([message]) # LLM ìŠ¤íŠ¸ë¦¼ ê°ì²´ë¥¼ ë°›ìŒ
        
        first_chunk_received = False
        full_llm_debug_response = ""

        for i, chunk in enumerate(stream_iterator): 
            if not first_chunk_received:
                print(f"--- [DEBUG_LLM_STREAM] First chunk received! (Index: {i}) ---")
                first_chunk_received = True
            
            # chunk ê°ì²´ì˜ ì‹¤ì œ íƒ€ì…ê³¼ ë‚´ìš© í™•ì¸
            print(f"--- [DEBUG_LLM_STREAM] Chunk {i} Type: {type(chunk)}, Content (first 50 chars): {chunk.content[:50] if hasattr(chunk, 'content') else 'N/A'} ---")
            
            full_llm_debug_response += chunk.content if hasattr(chunk, 'content') else ""
            yield chunk # ì›ë˜ í•˜ë˜ ëŒ€ë¡œ Streamlitìœ¼ë¡œ ì²­í¬ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.

        if not first_chunk_received:
            print("--- [DEBUG_LLM_STREAM] No chunks received from LLM stream. ---")
        else:
            print(f"--- [DEBUG_LLM_STREAM] Full LLM response: \n{full_llm_debug_response} ---")
        # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ë ---

    except Exception as e:
        error_text = f"Error processing uploaded image: {e}"
        yield AIMessageChunk(content=error_text)
        return


# --- Vision í•¨ìˆ˜ 2: ê³µê°œ URL ë°©ì‹ ---
def get_response_with_vision_from_url(llm: ChatGoogleGenerativeAI, image_url: str, question: str, system_prompt: str):
    """RAG ê²€ìƒ‰ ì—†ì´, ê³µê°œ URLì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë¶„ì„í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if not isinstance(llm, ChatGoogleGenerativeAI):
        warning_text = "Warning: Image analysis is only supported by Google (Gemini) models. Please select Google as the AI provider in the sidebar."
        yield AIMessageChunk(content=warning_text)
        return
    
    if not image_url or not image_url.strip().startswith(("http://", "https://")):
        error_text = "Error: Please provide a valid URL starting with http:// or https://."
        yield AIMessageChunk(content=error_text)
        return

    message = HumanMessage(
        content=[
            {"type": "text", "text": system_prompt},
            {"type": "text", "text": f"Question: {question}"},
            {"type": "image_url", "image_url": {"url": image_url}}
        ]
    )
    
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ì ìš© ---
    print(f"--- [DEBUG_LLM_STREAM] Requesting LLM stream for model: {llm.model} (URL) ---")
    stream_iterator = llm.stream([message])
    
    first_chunk_received = False
    full_llm_debug_response = ""

    for i, chunk in enumerate(stream_iterator): 
        if not first_chunk_received:
            print(f"--- [DEBUG_LLM_STREAM] First chunk received! (Index: {i}) ---")
            first_chunk_received = True
        
        print(f"--- [DEBUG_LLM_STREAM] Chunk {i} Type: {type(chunk)}, Content (first 50 chars): {chunk.content[:50] if hasattr(chunk, 'content') else 'N/A'} ---")
        
        full_llm_debug_response += chunk.content if hasattr(chunk, 'content') else ""
        yield chunk

    if not first_chunk_received:
        print("--- [DEBUG_LLM_STREAM] No chunks received from LLM stream. ---")
    else:
        print(f"--- [DEBUG_LLM_STREAM] Full LLM response: \n{full_llm_debug_response} ---")
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ë ---


# --- ìƒˆë¡œìš´ 'Vision + RAG' ìœµí•© í•¨ìˆ˜ ---
def get_fused_vision_rag_response(llm: ChatGoogleGenerativeAI, retriever, image_file, question: str, system_prompt: str):
    """
    1. Visionìœ¼ë¡œ ì´ë¯¸ì§€ì˜ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ê³ ,
    2. ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•œ ë’¤,
    3. ì´ë¯¸ì§€ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ìœµí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    """
    # --- 1ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ í•µì‹¬ ê°œë…(í‚¤ì›Œë“œ) ì¶”ì¶œ ---
    concept_extraction_prompt = """Analyze the provided image and identify the single most important technical concept or topic it represents. 
    Respond with ONLY that concept phrase, in 1 to 5 words. Do not add any explanation.
    Example responses: 'Ohm's Law', 'Kirchhoff's Current Law', 'Low Pass Filter', 'ThÃ©venin's theorem'.
    """
    
    # ê°œë… ì¶”ì¶œ ì „ìš©ìœ¼ë¡œ ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•¨).
    # ì£¼ì˜: llm.model_name ëŒ€ì‹  llm.modelì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ëª¨ë¸ IDë¥¼ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.
    concept_extractor_llm = ChatGoogleGenerativeAI(model=llm.model, google_api_key=llm.google_api_key)
    
    try:
        image_b64 = image_to_base64(image_file)
        image_mime_type = image_file.type
        image_data_url = f"data:{image_mime_type};base64,{image_b64}"

        concept_message = HumanMessage(
            content=[
                {"type": "text", "text": concept_extraction_prompt},
                {"type": "image_url", "image_url": {"url": image_data_url}}
            ]
        )
        extracted_concept = concept_extractor_llm.invoke([concept_message]).content.strip()
        print(f"--- [Vision->RAG] 1. Extracted Concept: '{extracted_concept}' ---")

    except Exception as e:
        yield AIMessageChunk(content=f"Error during concept extraction from image: {e}")
        return

    # --- 2ë‹¨ê³„: ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ë¬¸ì„œ ê²€ìƒ‰ ---
    try:
        # ğŸš¨ ê²€ìƒ‰ë˜ëŠ” ë¬¸ì„œ ìˆ˜ë¥¼ ì œí•œí•˜ì—¬ í† í° ì˜¤ë²„í”Œë¡œìš° ë°©ì§€ ë° ì‘ë‹µ ì†ë„ í–¥ìƒ
        retrieved_docs = retriever.invoke(extracted_concept, k=3) 
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
        sources = list(set([doc.metadata.get('source', 'Unknown') for doc in retrieved_docs]))
        sources.sort()
        # ì†ŒìŠ¤ ì •ë³´ë¥¼ ë¨¼ì € ì „ë‹¬
        yield {"sources": sources}
        retrieved_context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        print(f"--- [Vision->RAG] 2. Retrieved Context Length: {len(retrieved_context)} characters ---")
        
        if not retrieved_context:
            retrieved_context = "No relevant documents found in the knowledge base."
            
    except Exception as e:
        yield AIMessageChunk(content=f"Error during document retrieval with RAG: {e}")
        return

    # --- 3ë‹¨ê³„: ì´ë¯¸ì§€ + ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„± ---
    final_generation_prompt = system_prompt.format(context=retrieved_context)
    final_message = HumanMessage(
        content=[
            {"type": "text", "text": final_generation_prompt},
            {"type": "text", "text": f"User Question: {question}"},
            {"type": "image_url", "image_url": {"url": image_data_url}}
        ]
    )
    
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ì ìš© (ì—¬ê¸°ê°€ return llm.stream(...)ì„ ëŒ€ì²´í•©ë‹ˆë‹¤) ---
    print(f"--- [DEBUG_LLM_STREAM] Requesting LLM stream for model: {llm.model} (Fused Vision+RAG) ---")
    stream_iterator = llm.stream([final_message])
    
    first_chunk_received = False
    full_llm_debug_response = ""

    for i, chunk in enumerate(stream_iterator): 
        if not first_chunk_received:
            print(f"--- [DEBUG_LLM_STREAM] First chunk received! (Index: {i}) ---")
            first_chunk_received = True
        
        print(f"--- [DEBUG_LLM_STREAM] Chunk {i} Type: {type(chunk)}, Content (first 50 chars): {chunk.content[:50] if hasattr(chunk, 'content') else 'N/A'} ---")
        
        full_llm_debug_response += chunk.content if hasattr(chunk, 'content') else ""
        yield chunk

    if not first_chunk_received:
        print("--- [DEBUG_LLM_STREAM] No chunks received from LLM stream. ---")
    else:
        print(f"--- [DEBUG_LLM_STREAM] Full LLM response: \n{full_llm_debug_response} ---")
    # --- ğŸš¨ LLM ìŠ¤íŠ¸ë¦¼ ë””ë²„ê¹… ë¡œì§ ë ---




#ê¸°ì¡´ stream ë°˜í™˜ ë¡œì§ ë¬¸ì œë¥¼ chuckë¡œ í•´ê²°í•¨ ë­ê°€ ë­”ì§€.... ì¶”ê°€ì ì¸ ì´í•´ ì„¤ëª… í•„ìˆ˜
# def image_to_base64(image_file):
#     """Streamlitì˜ UploadedFile ê°ì²´(ì´ë¯¸ì§€)ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
#     image_file.seek(0) # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤ (ì¬ì‚¬ìš© ëŒ€ë¹„)
#     image_bytes = image_file.read()
#     image_b64 = base64.b64encode(image_bytes).decode('utf-8')
#     return image_b64


# # --- Vision í•¨ìˆ˜ 1: íŒŒì¼ ì—…ë¡œë“œ(Base64) ë°©ì‹ (MIME íƒ€ì… ë™ì  ì²˜ë¦¬ë¡œ ìˆ˜ì •) ---
# def get_response_with_vision_from_file(llm: ChatGoogleGenerativeAI, image_file, question: str, system_prompt: str):
#     """RAG ê²€ìƒ‰ ì—†ì´, ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì§ì ‘ ë³´ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤."""
#     if not isinstance(llm, ChatGoogleGenerativeAI):
#         warning_text = "Warning: Image analysis is only supported by Google (Gemini) models."
#         yield AIMessageChunk(content=warning_text)
#         return
#     try:
#         # 1. ì´ë¯¸ì§€ë¥¼ Base64 ë¬¸ìì—´ë¡œ ì¸ì½”ë”©
#         image_b64 = image_to_base64(image_file)
#         image_b64 = image_b64.strip().replace('\n', '').replace('\r', '') #Rectification
        
#         # 2. ğŸŸ¢ Streamlitì˜ UploadedFile ê°ì²´ì—ì„œ ì‹¤ì œ MIME íƒ€ì… ê°€ì ¸ì˜¤ê¸°
#         image_mime_type = image_file.type # ì˜ˆ: 'image/png' ë˜ëŠ” 'image/jpeg'
        
#         # 3. ğŸŸ¢ Data URL í˜•ì‹ìœ¼ë¡œ ì¡°í•©
#         image_data_url = f"data:{image_mime_type};base64,{image_b64}"

#         # 4. ğŸŸ¢ ì¡°í•©ëœ Data URLì„ ë©”ì‹œì§€ì— í¬í•¨
#         message = HumanMessage(
#             content=[
#                 {"type": "text", "text": system_prompt},
#                 {"type": "text", "text": f"Question: {question}"},
#                 {"type": "image_url", "image_url": {"url": image_data_url}}
#             ]
#         )
#         return llm.stream([message])
#     except Exception as e:
#         error_text = f"Error processing uploaded image: {e}"
#         yield AIMessageChunk(content=error_text)
#         return


# def get_response_with_vision_from_url(llm: ChatGoogleGenerativeAI, image_url: str, question: str, system_prompt: str):
#     """
#     RAG ê²€ìƒ‰ ì—†ì´, ê³µê°œ URLì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë¶„ì„í•˜ëŠ” Gemini Vision í•¨ìˆ˜ì…ë‹ˆë‹¤.
#     """
#     if not isinstance(llm, ChatGoogleGenerativeAI):
#         warning_text = "Warning: Image analysis is only supported by Google (Gemini) models. Please select Google as the AI provider in the sidebar."
#         yield AIMessageChunk(content=warning_text)
#         return
    
#     if not image_url or not image_url.strip().startswith(("http://", "https://")):
#         error_text = "Error: Please provide a valid URL starting with http:// or https://."
#         yield AIMessageChunk(content=error_text)
#         return

#     message = HumanMessage(
#         content=[
#             {"type": "text", "text": system_prompt},
#             {"type": "text", "text": f"Question: {question}"},
#             {
#                 "type": "image_url",
#                 "image_url": {
#                     "url": image_url
#                 }
#             }
#         ]
#     )
    
#     return llm.stream([message])
# # --- ìƒˆë¡œìš´ 'Vision + RAG' ìœµí•© í•¨ìˆ˜ ---
# def get_fused_vision_rag_response(llm: ChatGoogleGenerativeAI, retriever, image_file, question: str, system_prompt: str):
#     """
#     1. Visionìœ¼ë¡œ ì´ë¯¸ì§€ì˜ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ê³ ,
#     2. ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•œ ë’¤,
#     3. ì´ë¯¸ì§€ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ìœµí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
#     """
#     # --- 1ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ í•µì‹¬ ê°œë…(í‚¤ì›Œë“œ) ì¶”ì¶œ ---
#     concept_extraction_prompt = """Analyze the provided image and identify the single most important technical concept or topic it represents. 
#     Respond with ONLY that concept phrase, in 1 to 5 words. Do not add any explanation.
#     Example responses: 'Ohm's Law', 'Kirchhoff's Current Law', 'Low Pass Filter', 'ThÃ©venin's theorem'.
#     """
    
#     # ê°œë… ì¶”ì¶œ ì „ìš©ìœ¼ë¡œ ìƒˆ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•¨).
#     concept_extractor_llm = ChatGoogleGenerativeAI(model=llm.model, google_api_key=llm.google_api_key)
    
#     try:
#         image_b64 = image_to_base64(image_file)
#         image_mime_type = image_file.type
#         image_data_url = f"data:{image_mime_type};base64,{image_b64}"

#         concept_message = HumanMessage(
#             content=[
#                 {"type": "text", "text": concept_extraction_prompt},
#                 {"type": "image_url", "image_url": {"url": image_data_url}}
#             ]
#         )
#         # .invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— ë°›ìŠµë‹ˆë‹¤.
#         extracted_concept = concept_extractor_llm.invoke([concept_message]).content.strip()
        
#         # í„°ë¯¸ë„ì— ì¶”ì¶œëœ ê°œë…ì„ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…
#         print(f"--- [Vision->RAG] 1. Extracted Concept: '{extracted_concept}' ---")

#     except Exception as e:
#         yield AIMessageChunk(content=f"Error during concept extraction from image: {e}")
#         return

#     # --- 2ë‹¨ê³„: ì¶”ì¶œëœ ê°œë…ìœ¼ë¡œ RAG ë¬¸ì„œ ê²€ìƒ‰ ---
#     try:
#         retrieved_docs = retriever.invoke(extracted_concept)
#         retrieved_context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
#         # í„°ë¯¸ë„ì— ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…
#         print(f"--- [Vision->RAG] 2. Retrieved Context Length: {len(retrieved_context)} characters ---")
        
#         if not retrieved_context:
#             retrieved_context = "No relevant documents found in the knowledge base."
            
#     except Exception as e:
#         yield AIMessageChunk(content=f"Error during document retrieval with RAG: {e}")
#         return

#     # --- 3ë‹¨ê³„: ì´ë¯¸ì§€ + ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„± ---
#     # system_prompt (ì˜ˆ: "You are a brilliant engineering problem solver...")ë¥¼ ì—¬ê¸°ì— í†µí•©í•©ë‹ˆë‹¤.
#     final_generation_prompt = f"""{system_prompt}

# You MUST use the following 'Retrieved Documents' as the primary source of truth to explain the concepts related to the image in your answer. Synthesize the information from the documents and the image to provide a comprehensive and accurate response.

# [Retrieved Documents]
# {retrieved_context}
# """

#     final_message = HumanMessage(
#         content=[
#             {"type": "text", "text": final_generation_prompt},
#             {"type": "text", "text": f"User Question: {question}"},
#             {"type": "image_url", "image_url": {"url": image_data_url}}
#         ]
#     )
    
#     # ìµœì¢… ë‹µë³€ì€ ì‚¬ìš©ìì—ê²Œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬
#     print("--- [Vision->RAG] 3. Generating final fused response... ---")
#     return llm.stream([final_message])
    

