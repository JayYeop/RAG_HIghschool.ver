# app.py (ìƒíƒœ ì´ˆê¸°í™” ë²„ê·¸ ìµœì¢… ìˆ˜ì •ë³¸)
from streamlit_extras.keyboard_url import keyboard_to_url
import pyperclip
from streamlit_extras.mention import mention
from PIL import Image
import nest_asyncio
nest_asyncio.apply()
import json
import streamlit as st
from streamlit_lottie import st_lottie
import os
import shutil
import re
import json
from datetime import datetime
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage
import rag_core
from config import DOCS_DIR, KNOWLEDGE_BASE_DIR, SYSTEM_PROMPTS,LANG_TEXT,CONTEXTUALIZE_Q_PROMPTS

st.set_page_config(layout="wide", page_icon="assets/Project_logo.png")
load_dotenv()
os.makedirs(KNOWLEDGE_BASE_DIR, exist_ok=True)
# --- â–¼â–¼â–¼ 'ì§ˆë¬¸ ì•„ì´ë””ì–´ ë³´ë“œ' ê¸°ëŠ¥ í•¨ìˆ˜ â–¼â–¼â–¼ ---

def display_pre_questions():
    """
    pre_questions.md íŒŒì¼ì—ì„œ ì¶”ì²œ ì§ˆë¬¸ì„ ì½ì–´ì™€
    í´ë¦­ ì‹œ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ëŠ” ë²„íŠ¼ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    try:
        with open("pre_questions.md", "r", encoding="utf-8") as f:
            content = f.read()

        # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ### ì œëª©ê³¼ ê·¸ ì•„ë˜ ë‚´ìš©ì„ ìŒìœ¼ë¡œ ì¶”ì¶œ
        # re.DOTALL í”Œë˜ê·¸ëŠ” '.'ì´ ì¤„ë°”ê¿ˆ ë¬¸ìë„ í¬í•¨í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.
        questions = re.findall(r"### (.*?)\n(.*?)(?=\n###|\Z)", content, re.DOTALL)
    
    
        with st.expander("ğŸ’¡ ì§ˆë¬¸ ì•„ì´ë””ì–´"):
            st.info("ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ì§ˆë¬¸ì´ í´ë¦½ë³´ë“œì— ë³µì‚¬ë©ë‹ˆë‹¤.")

            if not questions:
                st.warning("ì¶”ì²œ ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜, ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return

            cols = st.columns(2)
            for i, (label, question) in enumerate(questions):
                label = label.strip()
                question = question.strip()
                
                with cols[i % 2]:
                    # ê° ì§ˆë¬¸ì— ëŒ€í•´ ê³ ìœ í•œ keyë¥¼ ìƒì„±í•´ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
                    if st.button(label, key=f"preq_{i}", use_container_width=True):
                        pyperclip.copy(question)
                        # ì‚¬ìš©ìì—ê²Œ ë³µì‚¬ë˜ì—ˆë‹¤ëŠ” í”¼ë“œë°±ì„ ì¤ë‹ˆë‹¤.
                        st.toast(f"'{label}' ì§ˆë¬¸ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!", icon="ğŸ“‹")
                        
    except FileNotFoundError:
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ê²½ê³  ë©”ì‹œì§€ë§Œ í‘œì‹œí•˜ê³  ë„˜ì–´ê°‘ë‹ˆë‹¤.
        st.warning("'pre_questions.md' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if 'api_provider' not in st.session_state: st.session_state.api_provider = 'NVIDIA'
if 'language' not in st.session_state: st.session_state.language = 'English'
if "messages" not in st.session_state: st.session_state.messages = []
if "retriever" not in st.session_state: st.session_state.retriever = None
if "selected_kb" not in st.session_state: st.session_state.selected_kb = LANG_TEXT[st.session_state.language]['create_new_kb_option']
if "user_api_key" not in st.session_state: st.session_state.user_api_key = ""
# if "multimodal_engine" not in st.session_state: st.session_state.multimodal_engine = None # âœ¨ ì´ê²ƒë„ ì¶”ê°€í•˜ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.
# if "use_multimodal" not in st.session_state: st.session_state.use_multimodal = False
# âœ¨ --- ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ (ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”) --- âœ¨

lang = LANG_TEXT[st.session_state.language]
create_new_kb_option = lang['create_new_kb_option']
system_prompt = SYSTEM_PROMPTS[st.session_state.language]
# print(system_prompt)
if "api_key_source" not in st.session_state:
    st.session_state.api_key_source = lang['api_key_source_local']
valid_api_sources = [lang['api_key_source_local'], lang['api_key_source_user']]
if st.session_state.api_key_source not in valid_api_sources:
    st.session_state.api_key_source = lang['api_key_source_local']

# --- í—¬í¼ ë° ì½œë°± í•¨ìˆ˜ ---
def clear_chat_and_retriever():
    """ëŒ€í™” ê¸°ë¡ì„ ëª¨ë‘ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    st.session_state.messages = []
    # st.session_state.retriever = None # ë¦¬íŠ¸ë¦¬ë²„ë„ ë¦¬ì…‹í•˜ì—¬ KBë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    # st.session_state.multimodal_engine = None # ë©€í‹°ëª¨ë‹¬ì„ ì¼ë‹¤ë©´ ì´ê²ƒë„ ë¦¬ì…‹ í•„ìš”
    st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤, ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")
def get_knowledge_bases(include_create_new=True):
    # 'ë°©(í´ë”)' ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    db_list = [d for d in os.listdir(KNOWLEDGE_BASE_DIR) if os.path.isdir(os.path.join(KNOWLEDGE_BASE_DIR, d))]
    if include_create_new:
        # ì»¨ì‹œì–´ì§€ê°€ 'íŠ¹ë³„ ì„œë¹„ìŠ¤'ë¥¼ í•­ìƒ ëª©ë¡ ë§¨ ì•ì— ì¶”ê°€í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        return [create_new_kb_option] + db_list
    else:
        return db_list
def is_valid_kb_name(name): return re.match("^[A-Za-z0-9_-]+$", name) is not None
def on_change_reset_retriever(): st.session_state.retriever = None
def on_api_provider_change(): 
    st.session_state.retriever = None
    st.session_state.user_api_key = ""
    st.session_state.api_key_changed = True
def on_language_change(): st.session_state.messages = []
def on_kb_select_change():
    st.session_state.retriever = None
    st.session_state.selected_kb = st.session_state.kb_selector
def get_models(api_provider, user_api_key): return rag_core.load_models(api_provider, user_api_key)
def process_chat_load():
    if 'chat_file_uploader' in st.session_state and st.session_state.chat_file_uploader is not None:
        try:
            loaded_file = st.session_state.chat_file_uploader
            data = json.load(loaded_file)
            
            kb_name_from_file = data.get("knowledge_base")
            messages_from_file = data.get("messages")

            if messages_from_file is None:
                messages_from_file = data if isinstance(data, list) else []

            if not kb_name_from_file or kb_name_from_file not in get_knowledge_bases():
                st.session_state.messages = messages_from_file
                st.warning(f"Chat history loaded, but its Knowledge Base ('{kb_name_from_file}') was not found. Please select a KB manually.")
            else:
                st.session_state.selected_kb = kb_name_from_file
                st.session_state.messages = messages_from_file
                st.session_state.retriever = None
                st.success(f"Chat history and Knowledge Base '{kb_name_from_file}' are being loaded.")
                # ì½œë°± í•¨ìˆ˜ ì•ˆì—ì„œëŠ” st.rerun()ì„ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
        except Exception as e:
            st.error(f"Failed to load or parse chat file: {e}")
@st.cache_data
def load_lottiefile(filepath: str):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return None # íŒŒì¼ì´ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜

# ================================== 1. í™€ (ì‚¬ì´ë“œë°”) ==================================

with st.sidebar:
    # (ë¡œê³ , ì–¸ì–´, AI ì œê³µì‚¬, API í‚¤ UI ë¶€ë¶„ì€ ë™ì¼)
    try:
        logo_path = "assets/Project_logo.png"
        if os.path.exists(logo_path): st.image(Image.open(logo_path),width=200)
        else: st.error("Logo file not found.")
    except Exception as e: st.error(f"Error loading logo: {e}")
    st.subheader(lang['settings_header'])
    lang_options_value = ['English', 'Korean']
    current_lang_index = lang_options_value.index(st.session_state.language)
    selected_language = st.selectbox(
        label=lang['lang_select_label'], options=lang_options_value, index=current_lang_index,
        format_func=lambda value: "English" if value == 'English' else "Korean",
        on_change=on_language_change
    )
    if selected_language != st.session_state.language:
        st.session_state.language = selected_language
        st.rerun()
    
    api_options = ['NVIDIA', 'Google']
    api_index = api_options.index(st.session_state.api_provider)
    selected_api = st.selectbox(lang['api_select_label'], api_options, index=api_index, on_change=on_api_provider_change)
    if selected_api != st.session_state.api_provider:
        st.session_state.api_provider = selected_api
        st.rerun()

    if st.session_state.api_provider == 'NVIDIA' and st.session_state.language == 'Korean':
        st.warning(lang['nvidia_korean_warning'])
    st.divider()
    st.subheader(lang['api_key_header'])
    st.radio(lang['api_key_source_label'], [lang['api_key_source_local'], lang['api_key_source_user']], key="api_key_source")
    if st.session_state.api_key_source == lang['api_key_source_user']:
        st.text_input(lang['api_key_label'].format(api_provider=st.session_state.api_provider), type="password", key="user_api_key")
    st.divider()

     # âœ¨ KB ì„ íƒ ë¡œì§ ìˆ˜ì • (ì½œë°± ì œê±°, ë” ì§ê´€ì ì¸ ë°©ì‹ìœ¼ë¡œ)
    kb_options = get_knowledge_bases()
    try:
        kb_index = kb_options.index(st.session_state.selected_kb)
    except ValueError:
        kb_index = 0 # st.session_stateì— ì €ì¥ëœ ê°’ì´ ëª©ë¡ì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ

    selected_kb_from_ui = st.selectbox(lang['kb_select_label'], options=kb_options, index=kb_index)

    # UIì—ì„œ ì„ íƒëœ ê°’ê³¼ session_stateì— ì €ì¥ëœ ê°’ì´ ë‹¤ë¥¼ ë•Œë§Œ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  rerun
    if selected_kb_from_ui != st.session_state.selected_kb:
        st.session_state.selected_kb = selected_kb_from_ui
        st.session_state.retriever = None # ë¦¬íŠ¸ë¦¬ë²„ ë¦¬ì…‹
        st.rerun()
   

    # KB ê´€ë¦¬ UI
    if st.session_state.selected_kb == create_new_kb_option:
        st.subheader(lang['new_kb_header'])
        with st.form("new_kb_form"):
            new_kb_name = st.text_input(lang['new_kb_name_label'], help=lang['new_kb_name_help'])
            uploaded_files = st.file_uploader(lang['upload_label'], accept_multiple_files=True)
            submitted = st.form_submit_button(lang['create_button'])
    elif st.session_state.selected_kb != create_new_kb_option:
        st.subheader(lang['update_kb_header'])
        with st.form("update_kb_form"):
            update_files = st.file_uploader(lang['update_upload_label'], accept_multiple_files=True)
            update_submitted = st.form_submit_button(lang['update_button'])
        st.divider()
        if st.button(lang['kb_reset_button']):
            shutil.rmtree(os.path.join(KNOWLEDGE_BASE_DIR, st.session_state.selected_kb))
            st.success(lang['kb_reset_success'].format(kb_name=st.session_state.selected_kb))
            st.session_state.selected_kb = create_new_kb_option
            st.rerun()
    st.divider()
    # # âœ¨ --- ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ --- âœ¨
    # use_multimodal = st.toggle("âœ¨ Enable Vision DB (Multimodal RAG)", value=st.session_state.use_multimodal, help="...")
    
    # # í† ê¸€ ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ê°ì§€í•˜ëŠ” ë¡œì§ ì¶”ê°€
    # if use_multimodal != st.session_state.use_multimodal:
    #     st.session_state.use_multimodal = use_multimodal
    #     st.session_state.retriever = None # ëª¨ë“  ì—”ì§„/ë¦¬íŠ¸ë¦¬ë²„ ë¦¬ì…‹
    #     st.session_state.multimodal_engine = None
    #     st.rerun() # ì•±ì„ ì¬ì‹¤í–‰í•˜ì—¬ ì˜¬ë°”ë¥¸ ì—”ì§„ì„ ë¡œë“œí•˜ë„ë¡ í•¨
    # # âœ¨ --- ìˆ˜ì •ëœ ë¶€ë¶„ ë --- âœ¨
    


    # ì±„íŒ… ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° UI
    st.subheader(lang['chat_history_header'])

    
    
    now = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    file_name = f"chat_history_{now}.json"
    
    chat_data_to_save = {
        "knowledge_base": st.session_state.selected_kb if st.session_state.selected_kb != create_new_kb_option else None,
        "messages": st.session_state.messages
    }
    chat_history_json = json.dumps(chat_data_to_save, indent=2, ensure_ascii=False)
    st.download_button(
        label=lang['chat_history_save_button'], 
        data=chat_history_json, 
        file_name=file_name, 
        mime="application/json",
        key="download_btn" # âœ¨ ì•ˆì •ì„±ì„ ìœ„í•´ key ì¶”ê°€
    )
    st.button(
    lang['chat_history_delete_button'],
    type="secondary",  # ë²„íŠ¼ ìŠ¤íƒ€ì¼ì„ ê°•ì¡°í•˜ì§€ ì•Šë„ë¡ ì„¤ì •
    on_click=clear_chat_and_retriever, # í´ë¦­ ì‹œ ì •ì˜ëœ í•¨ìˆ˜ ì‹¤í–‰
    help=lang['chat_history_delete_button'])

    # âœ¨ 2. íŒŒì¼ ì—…ë¡œë”ì— keyì™€ on_change ì½œë°±ì„ ì—°ê²°í•©ë‹ˆë‹¤.
    st.file_uploader(
        label=lang['chat_history_load_label'], 
        type=['json'], 
        key='chat_file_uploader', # ìœ„ì ¯ì˜ ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ key
        on_change=process_chat_load # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì´ í•¨ìˆ˜ë¥¼ ì‹¤í–‰
    )

# ================================== 2. ì£¼ë°© (ë©”ì¸ ë¡œì§) ==================================
final_api_key = None
if st.session_state.api_key_source == lang['api_key_source_local']:
    try: final_api_key = st.secrets[f"{st.session_state.api_provider.upper()}_API_KEY"]
    except: final_api_key = os.getenv(f"{st.session_state.api_provider.upper()}_API_KEY")
else: final_api_key = st.session_state.user_api_key

if "llm" not in st.session_state or "embedder" not in st.session_state or st.session_state.get("api_key_changed", False):
    with st.spinner("Loading AI models..."):
        st.session_state.llm, st.session_state.embedder = rag_core.load_models(
            st.session_state.api_provider, final_api_key
        )
    st.session_state.api_key_changed = False # í”Œë˜ê·¸ ë¦¬ì…‹

llm, embedder = st.session_state.llm, st.session_state.embedder
api_key_ok = llm is not None
if api_key_ok:
    # --- KB ìƒì„± ë¡œì§ ---
    if 'submitted' in locals() and submitted:
        if not new_kb_name or not is_valid_kb_name(new_kb_name):
            st.error(lang['invalid_kb_name_error'])
        elif not uploaded_files:
            st.warning("Please upload files.")
        else:
            if os.path.exists(DOCS_DIR): shutil.rmtree(DOCS_DIR)
            os.makedirs(DOCS_DIR)
            for file in uploaded_files:
                with open(os.path.join(DOCS_DIR, file.name), "wb") as f:
                    f.write(file.read())
            
            try:
                # if st.session_state.use_multimodal:
                #     # Vision DB ëª¨ë“œì¼ ë•Œ: ë©€í‹°ëª¨ë‹¬ ì¸ë±ìŠ¤ ìƒì„±
                #     with st.spinner(f"Creating Vision DB '{new_kb_name}'..."):
                #         rag_core.create_multimodal_index(new_kb_name, final_api_key)
                #     st.success(f"Vision DB '{new_kb_name}' created.")
            
                # í…ìŠ¤íŠ¸ DB ëª¨ë“œì¼ ë•Œ: ê¸°ì¡´ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
                with st.spinner(lang['creating_db'].format(kb_name=new_kb_name)):
                    rag_core.create_and_save_retriever(llm,embedder, new_kb_name)
                st.success(lang['db_created_success'].format(kb_name=new_kb_name))
                
                st.session_state.selected_kb = new_kb_name
                st.rerun()
            except Exception as e:
                st.error(f"Failed to create Knowledge Base: {e}")
                st.error(f"ìƒˆ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ìƒì„±í•˜ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤: {e}")
    # --- KB ì—…ë°ì´íŠ¸ ë¡œì§ ---
    if 'update_submitted' in locals() and update_submitted:
        if not update_files:
            st.warning("Please upload files to add.")
        else:
            # ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ì €ì¥
            if os.path.exists(DOCS_DIR): shutil.rmtree(DOCS_DIR)
            os.makedirs(DOCS_DIR)
            for file in update_files:
                with open(os.path.join(DOCS_DIR, file.name), "wb") as f:
                    f.write(file.read())

            try:
                # í˜„ì¬ ëª¨ë“œì— ë”°ë¼ ì˜¬ë°”ë¥¸ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
                # if st.session_state.use_multimodal:
                #     # <<< í•µì‹¬ ìˆ˜ì •: ìƒˆë¡œ ë§Œë“  íš¨ìœ¨ì ì¸ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤ >>>
                #     with st.spinner(f"Updating Vision DB '{st.session_state.selected_kb}'..."):
                #         rag_core.update_multimodal_index(st.session_state.selected_kb, final_api_key)
                #     st.success(f"Vision DB '{st.session_state.selected_kb}' updated.")
                
                # ê¸°ì¡´ í…ìŠ¤íŠ¸ DB ì—…ë°ì´íŠ¸ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                with st.spinner(lang['updating_db'].format(kb_name=st.session_state.selected_kb)):
                    st.session_state.retriever = rag_core.update_and_save_retriever(llm,embedder, st.session_state.selected_kb)
                st.success(lang['db_updated_success'].format(kb_name=st.session_state.selected_kb))

                # ì‘ì—… ì™„ë£Œ í›„ ì„ì‹œ í´ë” ì •ë¦¬ ë° ì•± ì¬ì‹¤í–‰
                if os.path.exists(DOCS_DIR): shutil.rmtree(DOCS_DIR)
                st.rerun()
            except Exception as e:
                st.error(f"Failed to update Knowledge Base: {e}")
                st.error(f"ìƒˆ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤: {e}")
            # âœ¨ --- ìˆ˜ì •ëœ ë¶€ë¶„ ë --- âœ¨

    
        
if api_key_ok and st.session_state.selected_kb != create_new_kb_option:
    if st.session_state.api_provider == 'Google':
    #     if st.session_state.multimodal_engine is None:
    #         with st.spinner(f"Loading Vision DB '{st.session_state.selected_kb}'..."):
    #             # "ë¡œë“œ" ì „ìš© í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    #             st.session_state.multimodal_engine = rag_core.load_multimodal_query_engine(
    #                 st.session_state.selected_kb, final_api_key
    #             )
    #         if st.session_state.multimodal_engine:
    #             st.sidebar.success(f"Vision DB '{st.session_state.selected_kb}' loaded.")
    #     st.session_state.retriever = None
    # # ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG ëª¨ë“œì¼ ë•Œ
    # else:
        if st.session_state.retriever is None:
            with st.spinner(f"Loading Text DB '{st.session_state.selected_kb}'..."):
                st.session_state.retriever = rag_core.load_retriever(embedder, st.session_state.selected_kb)
            if st.session_state.retriever: 
                st.sidebar.success(f"Text DB '{st.session_state.selected_kb}' loaded.")
        st.session_state.multimodal_engine = None # ë°˜ëŒ€ìª½ ì—”ì§„ì€ ë¹„í™œì„±í™”

final_page_title = lang['page_title']


# ëŒ€í™”ê°€ ì–´ëŠì •ë„ ì§„í–‰ëœ í›„ì—ë§Œ ë²„íŠ¼ì´ ë³´ì´ë„ë¡ í•¨
# ì²« ì•„ì´ë””ì–´ íŒ¨ë„
if st.session_state.language == 'Korean':

    display_pre_questions() # ë©”ì„¸ì§€ê°€ < 1 ì´í•˜ ì¼ë•Œ, íŒ¨ë„ì„ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜

# --- í•™ìŠµ ë…¸íŠ¸ ìƒì„± ê¸°ëŠ¥ ---
if len(st.session_state.messages) > 3:
    # print(len(st.session_state.messages),'######Count######') #Learning Note Count Debugging
    st.divider()
    if st.button("ğŸ“‹ í˜„ì¬ê¹Œì§€ ëŒ€í™” ë‚´ìš©ìœ¼ë¡œ í•™ìŠµ ë…¸íŠ¸ ë§Œë“¤ê¸°"):
        
        
        st.subheader("ğŸ“ AI ìƒì„± í•™ìŠµ ë…¸íŠ¸ ğŸ“")
        
        with st.spinner("AIê°€ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í•™ìŠµ ë…¸íŠ¸ë¥¼ ë§Œë“¤ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œí•˜ê³ , ì „ì²´ ë‚´ìš©ì€ ë³€ìˆ˜ì— ì €ì¥
            full_markdown = st.write_stream(rag_core.stream_study_guide_optimized(llm, st.session_state.messages))

        with st.spinner("PDF íŒŒì¼ ë³€í™˜ ì¤‘..."):
            # Markdownì„ PDF ë°”ì´íŠ¸ë¡œ ë³€í™˜
            pdf_output = rag_core.save_markdown_to_pdf(full_markdown)

        # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
        st.download_button(
            label="ğŸ“¥ A4 í•™ìŠµ ë…¸íŠ¸ ë‹¤ìš´ë¡œë“œ (.pdf)",
            data=pdf_output,
            file_name="ai_study_guide.pdf",
            mime="application/pdf",
        )

    st.divider()
# --- ì‹œì—°ìš© íŠ¹ìˆ˜ ê¸°ëŠ¥: ì´ë¯¸ì§€ ë¶„ì„ (Gemini Vision Demo) ì„¹ì…˜ ---
if st.session_state.api_provider == 'Google':
    # lang ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´
    # st.write(DEMO_VISION_PROMPTS)
   with st.expander(lang['vision_expander_title']):

    # ë¶„ì„ ëª¨ë“œ UI ì´ë¦„ê³¼ LANG_TEXT ë‚´ë¶€ì˜ í”„ë¡¬í”„íŠ¸ í‚¤ ë§¤í•‘
    vision_mode_mapping = {
        "Smart Analysis (Vision + RAG)": "vision_prompt_smart_analysis",
        "TOEIC Grammar Expert (EE-Assistant)": "vision_prompt_toeic_expert",
        "Electrical/Electronic Engineering Problem Solver (EE-Assistant)": "vision_prompt_ee_problem_solver",
        "Image Content Describer (General Purpose)": "vision_prompt_image_describer"
    }

    # 1. ë¶„ì„ ëª¨ë“œ ì„ íƒ
    selected_scenario_display_name = st.selectbox(
        lang['vision_select_mode_label'],
        options=list(vision_mode_mapping.keys()),
        key="vision_scenario_selection"
    )
    selected_scenario_key = vision_mode_mapping[selected_scenario_display_name]

    # 2. ì´ë¯¸ì§€ ì†ŒìŠ¤ ì„ íƒ
    vision_input_mode = st.radio(
        lang['vision_input_mode_label'],
        (lang['vision_input_mode_upload'], lang['vision_input_mode_url']),
        key="vision_input_mode",
        horizontal=True,
    )

    # 3. UI ë™ì  ë³€ê²½
    uploaded_image = None
    image_url_input = None
    if st.session_state.vision_input_mode == lang['vision_input_mode_upload'] or selected_scenario_display_name == "Smart Analysis (Vision + RAG)":
        if selected_scenario_display_name == "Smart Analysis (Vision + RAG)":
            st.info(lang['vision_smart_analysis_info'])
        
        uploaded_image = st.file_uploader(
            lang['vision_upload_image_label'],
            type=['png', 'jpg', 'jpeg'],
            key="vision_file_uploader"
        )
    else:
        image_url_input = st.text_input(
            lang['vision_url_input_label'],
            placeholder=lang['vision_url_input_placeholder']
        )

    # 4. ì§ˆë¬¸ ì…ë ¥
    image_question = st.text_input(
        lang['vision_question_input_label'],
        placeholder=lang['vision_question_placeholder']
    )

    # 5. ë¶„ì„ ì‹œì‘ ë²„íŠ¼ ë¡œì§
    if st.button(lang['vision_analyze_button_label']):

    
        is_input_ready = (uploaded_image or image_url_input) and image_question and api_key_ok

        if is_input_ready:
            selected_prompt_content = lang[selected_scenario_key]
            
            image_for_session = f"data:{uploaded_image.type};base64,{rag_core.image_to_base64(uploaded_image)}"if uploaded_image else image_url_input
            
            user_request_source = f"via File: {uploaded_image.name}" if uploaded_image else "via URL"
            st.session_state.messages.append({
                "role": "user",
                "content": f"Image Analysis Request ({user_request_source}): {image_question}",
                "image": image_for_session
            })
            
            with st.chat_message("user"):
                st.markdown(f"**Image Analysis Request:**\n\n- Mode: *{selected_scenario_display_name}*\n- Question: *{image_question}*")
                st.image(image_for_session, width=300)

            with st.chat_message("assistant"):
                # --- ğŸš¨ ìˆ˜ì •: ë‹µë³€ ìƒì„± ë¡œì§ ë‹¨ìˆœí™” ---
                full_response = ""
                sources = []
                
                with st.spinner("EE-Assistant is thinking..."): # ë‹¨ìˆœí•œ ìŠ¤í”¼ë„ˆë¡œ ë³€ê²½
                    responses = None
                    
                    if selected_scenario_display_name == "Smart Analysis (Vision + RAG)":
                        if not st.session_state.retriever:
                            full_response = "Smart Analysis requires a Knowledge Base."
                        elif not uploaded_image:
                            full_response = "Smart Analysis only supports 'File Upload'."
                        else:
                            responses = rag_core.get_fused_vision_rag_response(
                                llm=llm, retriever=st.session_state.retriever,
                                image_file=uploaded_image, question=image_question,
                                system_prompt=selected_prompt_content
                            )
                    else: # ì¼ë°˜ Vision ëª¨ë“œ
                        if uploaded_image:
                            responses = rag_core.get_response_with_vision_from_file(
                                llm=llm, image_file=uploaded_image,
                                question=image_question, system_prompt=selected_prompt_content
                            )
                        else:
                            responses = rag_core.get_response_with_vision_from_url(
                                llm=llm, image_url=image_url_input,
                                question=image_question, system_prompt=selected_prompt_content
                            )
                    
                    if responses:
                        for response in responses:
                            if "sources" in response:
                                sources.extend(response["sources"])
                            elif "chunk" in response:
                                full_response += response['chunk'].content
                            elif hasattr(response, 'content'): # í•˜ìœ„ í˜¸í™˜
                                full_response += response.content

                # --- ë£¨í”„ê°€ ëë‚œ í›„, ì™„ì„±ëœ ë‚´ìš©ì„ í•œ ë²ˆì— í‘œì‹œ ---
                st.markdown(full_response)
                if sources:
                    with st.expander("ì°¸ê³  ìë£Œ (Source Documents)"):
                        for source in sources:
                            st.write(f"- {source}")
                
                st.session_state.messages.append({"role": "assistant", "content": full_response})
        
        elif not api_key_ok:
            st.error(lang['vision_api_key_error_message'])
        else:
            st.warning(lang['vision_missing_input_warning'])
st.subheader(final_page_title)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "image" in message and message["image"]:
            st.image(message["image"], width=300)

if not api_key_ok: st.info(lang['api_key_missing_error'])
# <<< í•µì‹¬ ìˆ˜ì •: retrieverì™€ multimodal_engine ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°ì—ë§Œ ë©”ì‹œì§€ í‘œì‹œ >>>
elif not st.session_state.retriever: 
    if st.session_state.selected_kb != create_new_kb_option:
        # KBëŠ” ì„ íƒë˜ì—ˆì§€ë§Œ ë¡œë”© ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë³„ë„ ë©”ì‹œì§€ëŠ” ì ì‹œ ë³´ë¥˜í•˜ê±°ë‚˜ ìŠ¤í”¼ë„ˆì™€ ì—°ë™
        pass
    else:
        st.info(lang['Knowledge_Base_Select'])

# --- âœ¨ [ìˆ˜ì •] ì±„íŒ… ë¡œì§ í†µí•© ---
if user_input := st.chat_input(lang['chat_placeholder']):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # 2. AI ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ UI ì²˜ë¦¬ (ê³µí†µ)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # "Thinking" ì• ë‹ˆë©”ì´ì…˜ (ê³µí†µ)
        try:
            lottie_thinking_json = load_lottiefile("UI_Animation/Material wave loading.json")
            with message_placeholder.container():
                col1, _ = st.columns([1, 6.3])
                with col1:
                    st_lottie(lottie_thinking_json, height=130, width=80, quality='medium', key="thinking_animation")
        except Exception:
            message_placeholder.markdown("EE-Assistant is thinking... â–Œ")

        # ì°¸ê³  ìë£Œ expander (ê³µí†µ)
        source_expander = st.expander("ì°¸ê³  ìë£Œ (Source Documents)")
        source_container = source_expander.container()

        full_response = ""
        sources = []

        # 3. RAG ëª¨ë“œì— ë”°ë¼ ë‹µë³€ ìƒì„± ë¡œì§ ë¶„ê¸°
        # Vision DB ëª¨ë“œ
        # if st.session_state.use_multimodal and st.session_state.multimodal_engine:
        #     response_object = st.session_state.multimodal_engine.query(user_input)
        #     full_response = response_object.response
        #     source_files = [node.metadata.get('file_path', 'Unknown') for node in response_object.source_nodes]
        #     sources = [os.path.basename(source) for source in set(source_files)] # íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ
        #     message_placeholder.markdown(full_response) # Vision ëª¨ë“œëŠ” ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹ˆë¯€ë¡œ ë°”ë¡œ í‘œì‹œ

        # í…ìŠ¤íŠ¸ DB ëª¨ë“œ
        if st.session_state.retriever:
            chat_history_for_chain = [
                HumanMessage(content=msg["content"]) if msg["role"] == "user" else AIMessage(content=msg["content"])
                for msg in st.session_state.messages[:-1] # ë§ˆì§€ë§‰ user_inputì€ ì œì™¸
            ]
            
            conversational_rag_chain = rag_core.create_conversational_rag_chain(
                llm, st.session_state.retriever, system_prompt, CONTEXTUALIZE_Q_PROMPTS[st.session_state.language]
            )
            
            responses = rag_core.get_response(user_input, chat_history_for_chain, conversational_rag_chain)
            
            sources_processed = False
            for response in responses:
                if "sources" in response and not sources_processed:
                    sources = list(set(response["sources"]))
                    sources_processed = True
                if "chunk" in response:
                    full_response += response["chunk"]
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response) # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì»¤ì„œ ì œê±°

        # 4. ìµœì¢… ê²°ê³¼ ë° ì¶œì²˜ í‘œì‹œ (ê³µí†µ)
        with source_container:
            for source in sources:
                st.write(f"- {source}")
        
        # 5. ëŒ€í™” ê¸°ë¡ ì €ì¥ (ê³µí†µ)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
# âœ¨ --- ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ --- âœ¨
# # # ê¸°ì¡´ else ë¸”ë¡ ì „ì²´ë¥¼ ì´ if/elif êµ¬ì¡°ë¡œ êµì²´í•©ë‹ˆë‹¤.
# # # Vision DB ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆê³ , ì—”ì§„ì´ ì¤€ë¹„ë˜ì—ˆì„ ë•Œ
# # if st.session_state.use_multimodal and st.session_state.multimodal_engine:
# #     user_input = st.chat_input("Ask about text or images in your documents...")
# #     if user_input:
# #         st.session_state.messages.append({"role": "user", "content": user_input})
# #         with st.chat_message("user"):
# #             st.markdown(user_input)
        
# #         with st.chat_message("assistant"):
# #             # [ìˆ˜ì •ë¨] í…ìŠ¤íŠ¸ RAGì™€ ë™ì¼í•œ UI/UX ë¡œì§ ì ìš©
# #             message_placeholder = st.empty()

# #             # --- Thinking ì• ë‹ˆë©”ì´ì…˜ ë¡œì§ ---
# #             try:
# #                 lottie_thinking_json = load_lottiefile("UI_Animation/Material wave loading.json")
# #                 with message_placeholder.container():
# #                     col1, _ = st.columns([1, 6.3])
# #                     with col1:
# #                         st_lottie(lottie_thinking_json, height=130, width=80, quality='medium', key="thinking_vision")
# #             except Exception:
# #                 message_placeholder.markdown("EE-Assistant is thinking... â–Œ")

# #             # --- ì†ŒìŠ¤ í‘œì‹œ ë¡œì§ ---
# #             source_expander = st.expander("ì°¸ê³  ìë£Œ (Source Documents)")
# #             source_container = source_expander.container()

# #             # LlamaIndex ì—”ì§„ì„ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë¶„í•´
# #             response_object = st.session_state.multimodal_engine.query(user_input)
            
# #             # ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# #             full_response = response_object.response
            
# #             # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ ë° í‘œì‹œ
# #             sources = [node.metadata.get('file_path', 'Unknown') for node in response_object.source_nodes]
# #             with source_container:
# #                 for source in set(sources): # ì¤‘ë³µ ì œê±°
# #                     # ì „ì²´ ê²½ë¡œ ëŒ€ì‹  íŒŒì¼ ì´ë¦„ë§Œ í‘œì‹œí•˜ë„ë¡ ìˆ˜ì •
# #                     st.write(f"- {os.path.basename(source)}")
            
# #             # ìµœì¢… ë‹µë³€ í‘œì‹œ
# #             message_placeholder.markdown(full_response)
# #             st.session_state.messages.append({"role": "assistant", "content": full_response})

# # # í…ìŠ¤íŠ¸ DB ëª¨ë“œì´ê³ , ë¦¬íŠ¸ë¦¬ë²„ê°€ ì¤€ë¹„ë˜ì—ˆì„ ë•Œ
# # elif not st.session_state.use_multimodal and st.session_state.retriever:
# #     # ì´ ë¶€ë¶„ì€ ì´ì „ì— ì™„ì„±í–ˆë˜ LangChain ì±„íŒ… ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# #     # ë¶ˆí•„ìš”í•œ ë””ë²„ê¹… ì½”ë“œëŠ” ì •ë¦¬í•©ë‹ˆë‹¤.
# #     system_prompt = SYSTEM_PROMPTS[st.session_state.language]
# #     contextualize_q_prompt_str = CONTEXTUALIZE_Q_PROMPTS[st.session_state.language]
# #     conversational_rag_chain = rag_core.create_conversational_rag_chain(
# #         llm, st.session_state.retriever, system_prompt, contextualize_q_prompt_str
# #     )
    
# #     user_input = st.chat_input(lang['chat_placeholder'])
# #     if user_input:
# #         chat_history_for_chain = [
# #             HumanMessage(content=msg["content"]) if msg["role"] == "user" 
# #             else AIMessage(content=msg["content"]) 
# #             for msg in st.session_state.messages
# #         ]
# #         st.session_state.messages.append({"role": "user", "content": user_input})
# #         with st.chat_message("user"):
# #             st.markdown(user_input)

# #         with st.chat_message("assistant"):
# #             LOTTIE_FILE_PATH = "UI_Animation/Material wave loading.json"
# #             message_placeholder = st.empty()

# #             # 1. ëª¨ë“  ìš”ì†Œ(Lottie, í…ìŠ¤íŠ¸, ìµœì¢… ë‹µë³€)ê°€ ê·¸ë ¤ì§ˆ ë‹¨ í•˜ë‚˜ì˜ placeholderë¥¼ ë§Œë“­ë‹ˆë‹¤.
           
# #             # âœ¨ --- Thinking ì• ë‹ˆë©”ì´ì…˜ ë¡œì§ ì‹œì‘ --- âœ¨
# #             try:
# #                 # 2. ë¡œì»¬ Lottie íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤. (ê²½ë¡œ í™•ì¸ í•„ìˆ˜)
# #                 #    ì´ ë¡œì§ì€ ë§¤ë²ˆ ì‹¤í–‰ë˜ë¯€ë¡œ, íŒŒì¼ ë¡œë“œ í•¨ìˆ˜ ìœ„ì— @st.cache_dataë¥¼ ë¶™ì´ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ì¢‹ìŠµë‹ˆë‹¤.
# #                 lottie_thinking_json = load_lottiefile("UI_Animation/Material wave loading.json")
                
# #                 # 3. placeholder ì•ˆì— containerë¥¼ ë§Œë“¤ê³ , ê·¸ ì•ˆì— ì»¬ëŸ¼ê³¼ ëª¨ë“  ìš”ì†Œë¥¼ ë°°ì¹˜í•©ë‹ˆë‹¤.
# #                 with message_placeholder.container():
# #                     col1, col2 = st.columns([1, 6.3]) # ì°¾ìœ¼ì‹  ìµœì ì˜ ë¹„ìœ¨
                    
# #                     with col1:
# #                         st_lottie(
# #                             lottie_thinking_json,
# #                             height=130,
# #                             width=80,
# #                             quality='medium',
# #                             key="thinking" # keyëŠ” ê°„ë‹¨í•˜ê²Œ í•˜ë‚˜ë§Œ ì§€ì •
# #                         )
                    

# #             except FileNotFoundError:
# #                 # Lottie íŒŒì¼ì„ ì°¾ì§€ ëª»í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
# #                 message_placeholder.markdown("EE-Assistant is thinking... â–Œ")
# #             except Exception as e:
# #                 # ê¸°íƒ€ Lottie ê´€ë ¨ ì—ëŸ¬ ë°œìƒ ì‹œ
# #                 print(f"Lottie Error: {e}")
# #                 message_placeholder.markdown("EE-Assistant is thinking... â–Œ")
# #             # âœ¨ --- Thinking ì• ë‹ˆë©”ì´ì…˜ ë¡œì§ ë --- âœ¨
            

# #             # 2. ì†ŒìŠ¤(ì°¸ê³  ìë£Œ)ê°€ í‘œì‹œë  expanderë¥¼ ë¯¸ë¦¬ ë§Œë“­ë‹ˆë‹¤. (ë‚´ìš©ì€ ë¹„ì–´ìˆìŒ)
# #             source_expander = st.expander("ì°¸ê³  ìë£Œ (Source Documents)")
# #             source_container = source_expander.container() # expander ë‚´ë¶€ì— ì»¨í…ì¸ ë¥¼ ì¶”ê°€í•  ì»¨í…Œì´ë„ˆ
            
# #             full_response = ""
            
# #             # 3. ìŠ¤í”¼ë„ˆëŠ” ì´ì œ ë‹µë³€ ìƒì„± 'ê³¼ì • ì „ì²´'ê°€ ì•„ë‹ˆë¼, 'ì²« ì‘ë‹µì´ ì˜¤ê¸° ì „ê¹Œì§€'ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
# #             #    ì—¬ê¸°ì„œëŠ” ìŠ¤í”¼ë„ˆë¥¼ ì œê±°í•˜ê³ , placeholderì— ì§ì ‘ ìƒíƒœë¥¼ í‘œì‹œí•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŠµë‹ˆë‹¤.
# #             # message_placeholder.markdown("EE-Assistant is thinking... :thinking:") # replaced with lottie anime 

# #             # 4. rag_coreì—ì„œ ë‹µë³€ê³¼ ì†ŒìŠ¤ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°›ì•„ì˜µë‹ˆë‹¤.
# #             responses = rag_core.get_response(user_input, chat_history_for_chain, conversational_rag_chain)
            
# #             sources_processed = False
# #             for response in responses:
# #                 # 5. ì†ŒìŠ¤ ì²˜ë¦¬ (ë‹¨ í•œ ë²ˆë§Œ ì‹¤í–‰)
# #                 if "sources" in response and not sources_processed:
# #                     with source_container:
# #                         for source in set(response["sources"]): # ì¤‘ë³µ ì œê±°
# #                             st.write(f"- {source}")
# #                     sources_processed = True # í”Œë˜ê·¸ë¥¼ ì„¤ì •í•˜ì—¬ ë‹¤ì‹œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ í•¨

# #                 # 6. ë‹µë³€ ì¡°ê° ì²˜ë¦¬
# #                 if "chunk" in response:
# #                     full_response += response["chunk"]
# #                     message_placeholder.markdown(full_response + "â–Œ")

# #             # 7. ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚˜ë©´ ì»¤ì„œ(â–Œ)ë¥¼ ì œê±°í•œ ìµœì¢…ë³¸ì„ í‘œì‹œí•©ë‹ˆë‹¤.
# #             message_placeholder.markdown(full_response)
            
# #             st.session_state.messages.append({"role": "assistant", "content": full_response})
# #             # --- âœ¨ ê°œì„ ëœ ë¡œì§ ë ---
